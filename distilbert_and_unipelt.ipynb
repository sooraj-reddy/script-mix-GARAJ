{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "MesYMW9uErQt",
        "l7Qg1my2Eyls",
        "88e9EPDLo5Ya",
        "JAAIa9wXX0Oo",
        "VkT30c1oYGFB",
        "RjfnP0X7YMVJ",
        "rbMcrRnXJ0YU",
        "65wSNa6py0Xu",
        "nZrjCyYKkw0m",
        "5FJBNtrAAS_Q",
        "GtCkWdHvAa7H",
        "s6WYf2yvAl5p"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Replace 'your_folder' with the folder you want to download\n",
        "folder_path = \"/content/la_s\"\n",
        "zip_path = \"/content/la_s.zip\"\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive(zip_path.replace(\".zip\", \"\"), 'zip', folder_path)\n",
        "\n",
        "# Download the zipped folder\n",
        "from google.colab import files\n",
        "files.download(zip_path)"
      ],
      "metadata": {
        "id": "qwynPrMREPy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Replace 'your_folder' with the folder you want to download\n",
        "folder_path = \"/content/la_t\"\n",
        "zip_path = \"/content/la_t.zip\"\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive(zip_path.replace(\".zip\", \"\"), 'zip', folder_path)\n",
        "\n",
        "# Download the zipped folder\n",
        "from google.colab import files\n",
        "files.download(zip_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "LaC9IkGCEyrT",
        "outputId": "3e09c775-f080-43b0-af15-c94eeee38e8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9ba9e5ea-0578-4506-8e7d-d30b366d076e\", \"la_t.zip\", 3009507)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ],
      "metadata": {
        "id": "MesYMW9uErQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn_crfsuite"
      ],
      "metadata": {
        "id": "z7WRPU7psbUt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26ed3c7d-d72a-4980-f7f3-a086decc432b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sklearn_crfsuite in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.7 in /usr/local/lib/python3.11/dist-packages (from sklearn_crfsuite) (0.9.11)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from sklearn_crfsuite) (1.6.1)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from sklearn_crfsuite) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.11/dist-packages (from sklearn_crfsuite) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->sklearn_crfsuite) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/adapter-hub/adapters.git\n",
        "!cd adapters"
      ],
      "metadata": {
        "id": "tTd8C4rNcFFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43a179b2-746a-4d09-9588-7e26598e3cad",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'adapters' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets conllu"
      ],
      "metadata": {
        "id": "CUcFjhZjZPxG",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fb48bac-f2d2-487b-b515-45a918027eb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.11/dist-packages (6.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers datasets accelerate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4obIiehjwoaZ",
        "outputId": "5df8b389-0100-4a3f-deb1-c0e7f2876ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install adapters"
      ],
      "metadata": {
        "id": "rxhT68FmZ083",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b1c6dbf-1d55-4052-f3f1-5ab8a1ceb020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: adapters in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: transformers~=4.48.3 in /usr/local/lib/python3.11/dist-packages (from adapters) (4.48.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from adapters) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.48.3->adapters) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers~=4.48.3->adapters) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers~=4.48.3->adapters) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers~=4.48.3->adapters) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers~=4.48.3->adapters) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers~=4.48.3->adapters) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers~=4.48.3->adapters) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from adapters import AutoAdapterModel\n",
        "from transformers import AutoTokenizer\n",
        "from adapters import AdapterTrainer\n",
        "from transformers import TrainingArguments\n",
        "from transformers import AutoConfig\n",
        "from adapters import AdapterConfig\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from datasets import Dataset, concatenate_datasets"
      ],
      "metadata": {
        "id": "6w0DSzzgclYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir Ug/"
      ],
      "metadata": {
        "id": "RKXAmy01j4Uq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "012c8173-3881-47ec-de38-4b03695b439d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘Ug/’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h99oQP5dZDvG"
      },
      "outputs": [],
      "source": [
        "import conllu\n",
        "\n",
        "def extract_corpus(conllu_path):\n",
        "    \"\"\"Extracts transliterated sentences from a CONLLU file.\"\"\"\n",
        "    translit_sentences = []\n",
        "    original_sentences = []\n",
        "    with open(conllu_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = conllu.parse(f.read())\n",
        "\n",
        "    for sentence in data:\n",
        "        original = sentence.metadata.get('text', '')\n",
        "        translit = sentence.metadata.get('translit', '')\n",
        "        if translit:\n",
        "            translit_sentences.append(translit.strip())\n",
        "        if original:\n",
        "            original_sentences.append(original.strip())\n",
        "    return original_sentences, translit_sentences\n",
        "\n",
        "def original_corpus(files):\n",
        "    corpus = {}\n",
        "    for split, file_path in files.items():\n",
        "        original, translit = extract_corpus(file_path)\n",
        "        corpus[split] = {\"original\": original, \"translit\": translit}\n",
        "\n",
        "    for split in [\"train\", \"dev\", \"test\"]:\n",
        "        with open(f\"ug_{split}_original.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"\\n\".join(corpus[split][\"original\"]) + \"\\n\")\n",
        "        with open(f\"ug_{split}_transliterated.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"\\n\".join(corpus[split][\"translit\"]) + \"\\n\")\n",
        "\n",
        "    return corpus[\"train\"][\"original\"], corpus[\"train\"][\"translit\"]\n",
        "\n",
        "files = {\n",
        "        \"train\": \"ug_udt-ud-train.conllu\",\n",
        "        \"dev\": \"ug_udt-ud-dev.conllu\",\n",
        "        \"test\": \"ug_udt-ud-test.conllu\"\n",
        "    }\n",
        "original_texts, transliterated_texts = original_corpus(files)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_texts[4]"
      ],
      "metadata": {
        "id": "q1sJTdVFn3wg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "dc9a0bdd-2f81-43a5-d4da-33dde502e52e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'نەشپۈت كۆچىتىنى يۇلىۋېتىپ ئۆرۈك كۆچىتى تىكىپتۇ.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transliterated_texts[4]"
      ],
      "metadata": {
        "id": "f33K2u3-n5C6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "b56d5aab-81b9-4c6e-b91c-bb8656ade414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'neshpüt köchitini yuliwëtip örük köchiti tikiptu.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "additional_tokens = [\"<new_token1>\", \"<new_token2>\"]\n",
        "tokenizer.add_tokens(additional_tokens)\n",
        "\n",
        "def tokenize(text):\n",
        "    return tokenizer(\n",
        "        text,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )"
      ],
      "metadata": {
        "id": "tJWc3K6hZOCk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d71cba-c729-446d-b6a0-c7e6a4d9dd34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Force CPU-only mode\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # For better error messages\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from adapters import AutoAdapterModel, Stack, AdapterConfig\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling\n",
        "\n",
        "# ========== Device Setup ==========\n",
        "# device = torch.device(\"cpu\")  # Force CPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ========== Keep Original Dataset Intact ==========\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts):\n",
        "        self.texts = texts\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\"text\": self.texts[idx]}\n",
        "\n",
        "# ========== Model Loading with CPU Enforcement ==========\n",
        "teacher_model = AutoModelForMaskedLM.from_pretrained(\n",
        "    \"coppercitylabs/uzbert-base-uncased\"\n",
        ").to(device).eval()\n",
        "\n",
        "model = AutoAdapterModel.from_pretrained(\n",
        "    \"distilbert-base-multilingual-cased\"\n",
        ").to(device)\n",
        "\n",
        "# ========== Tokenizers ==========\n",
        "teacher_tokenizer = AutoTokenizer.from_pretrained(\"coppercitylabs/uzbert-base-uncased\")\n",
        "student_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
        "\n",
        "# ========== Adapter Setup (with device enforcement) ==========\n",
        "lang_config = AdapterConfig.load(\n",
        "    \"pfeiffer\",\n",
        "    reduction_factor=16,\n",
        "    leave_out=[5]\n",
        ")\n",
        "\n",
        "# Add adapters and force device placement\n",
        "model.add_adapter(\"la_s\", config=lang_config)\n",
        "model.add_adapter(\"la_t\", config=lang_config)\n",
        "model = model.to(device)  # Critical: Re-affirm device placement after adding adapters\n",
        "model.train_adapter([\"la_s\", \"la_t\"])\n",
        "\n",
        "# ========== Verify Adapter Device Placement ==========\n",
        "# Add debug checks for adapter parameters\n",
        "# print(\"\\nDevice verification:\")\n",
        "# for name, param in model.named_parameters():\n",
        "#     if 'la_s' in name or 'la_t' in name:\n",
        "#         print(f\"{name}: {param.device}\")\n",
        "\n",
        "# ========== Data Collator ==========\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=student_tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=0.15\n",
        ")\n",
        "\n",
        "# ========== Training Utilities ==========\n",
        "\n",
        "\n",
        "def safe_tokenize(texts, tokenizer, is_student=False):\n",
        "    \"\"\"Handles tokenization with proper device placement\"\"\"\n",
        "    if is_student:\n",
        "        # Tokenize on CPU first\n",
        "        tokens = tokenizer(\n",
        "            texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=512\n",
        "        )\n",
        "\n",
        "        # Collate on CPU\n",
        "        masked = data_collator([{\"input_ids\": ids} for ids in tokens[\"input_ids\"]])\n",
        "\n",
        "        # Move all tensors to device\n",
        "        return {k: v.to(device) for k, v in masked.items()}\n",
        "    else:\n",
        "        # Teacher processing stays on GPU\n",
        "        tokens = tokenizer(\n",
        "            texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=512\n",
        "        ).to(device)\n",
        "        return tokens\n",
        "\n",
        "\n",
        "def distillation_loss(s_repr, t_repr):\n",
        "    return F.mse_loss(s_repr, t_repr)\n",
        "\n",
        "# ========== Training Loop ==========\n",
        "def train_adapter(adapter_name, dataloader, num_epochs=10):\n",
        "    model.train_adapter(adapter_name)\n",
        "    model.set_active_adapters(Stack(adapter_name))\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in dataloader:\n",
        "            texts = batch[\"text\"]\n",
        "\n",
        "            # Student processing\n",
        "            student_inputs = safe_tokenize(texts, student_tokenizer, is_student=True)\n",
        "\n",
        "            # Teacher processing\n",
        "            with torch.no_grad():\n",
        "                teacher_inputs = safe_tokenize(texts, teacher_tokenizer)\n",
        "                teacher_outputs = teacher_model(**teacher_inputs, output_hidden_states=True)\n",
        "                t_repr = teacher_outputs.hidden_states[-1][:, 0, :]\n",
        "\n",
        "            # print(f\"Model device: {next(model.parameters()).device}\")\n",
        "            # print(f\"Input IDs device: {student_inputs['input_ids'].device}\")\n",
        "            # print(f\"Attention mask device: {student_inputs['attention_mask'].device}\")\n",
        "\n",
        "            # Student forward\n",
        "            student_outputs = model(\n",
        "                input_ids=student_inputs[\"input_ids\"],\n",
        "                attention_mask=student_inputs[\"attention_mask\"],\n",
        "                output_hidden_states=True\n",
        "            )\n",
        "            s_repr = student_outputs.hidden_states[-1][:, 0, :]\n",
        "\n",
        "            # Loss computation\n",
        "            loss = distillation_loss(s_repr, t_repr)\n",
        "            loss.backward()\n",
        "            # optimizer.step()\n",
        "            # optimizer.zero_grad()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | Loss: {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "\n",
        "original_dataset = TextDataset(original_texts)\n",
        "transliterated_dataset = TextDataset(transliterated_texts)\n",
        "\n",
        "original_dataloader = DataLoader(original_dataset, batch_size=16, shuffle=True)\n",
        "transliterated_dataloader = DataLoader(transliterated_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# ========== Train ==========\n",
        "print(\"Training la_s adapter...\")\n",
        "train_adapter(\"la_s\", original_dataloader, num_epochs=10)\n",
        "\n",
        "print(\"Training la_t adapter...\")\n",
        "train_adapter(\"la_t\", transliterated_dataloader, num_epochs=10)\n",
        "\n",
        "# ========== Save ==========\n",
        "model.save_adapter(\"./la_s\", \"la_s\", with_head=False)\n",
        "model.save_adapter(\"./la_t\", \"la_t\", with_head=False)\n",
        "print(\"Adapters saved successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjc-RnqgbKrh",
        "outputId": "172d92e2-c05a-4acf-84e0-1970043e11b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/adapters/composition.py:225: FutureWarning: Passing list objects for adapter activation is deprecated. Please use Stack or Fuse explicitly.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training la_s adapter...\n",
            "Epoch 1 | Loss: 1.1347\n",
            "Epoch 2 | Loss: 1.1293\n",
            "Epoch 3 | Loss: 1.1390\n",
            "Epoch 4 | Loss: 1.1342\n",
            "Epoch 5 | Loss: 1.1312\n",
            "Epoch 6 | Loss: 1.1323\n",
            "Epoch 7 | Loss: 1.1268\n",
            "Epoch 8 | Loss: 1.1380\n",
            "Epoch 9 | Loss: 1.1327\n",
            "Epoch 10 | Loss: 1.1268\n",
            "Training la_t adapter...\n",
            "Epoch 1 | Loss: 1.1076\n",
            "Epoch 2 | Loss: 1.1082\n",
            "Epoch 3 | Loss: 1.1118\n",
            "Epoch 4 | Loss: 1.1039\n",
            "Epoch 5 | Loss: 1.1068\n",
            "Epoch 6 | Loss: 1.1087\n",
            "Epoch 7 | Loss: 1.1089\n",
            "Epoch 8 | Loss: 1.1104\n",
            "Epoch 9 | Loss: 1.1075\n",
            "Epoch 10 | Loss: 1.1080\n",
            "Adapters saved successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7O_g_3dQflr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "l7Qg1my2Eyls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  adapter configurations\n",
        "print(\"\\nla_s config:\", model.get_adapter(\"la_s\"))\n",
        "print(\"la_t config:\", model.get_adapter(\"la_t\"))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uY2_IddqhLrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a87a5a6-94f4-4da0-9e74-183428c8f143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "la_s config: {0: {'output_adapter': Adapter(\n",
            "  (non_linearity): Activation_Function_Class(\n",
            "    (f): ReLU()\n",
            "  )\n",
            "  (adapter_down): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=48, bias=True)\n",
            "    (1): Activation_Function_Class(\n",
            "      (f): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")}, 1: {'output_adapter': Adapter(\n",
            "  (non_linearity): Activation_Function_Class(\n",
            "    (f): ReLU()\n",
            "  )\n",
            "  (adapter_down): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=48, bias=True)\n",
            "    (1): Activation_Function_Class(\n",
            "      (f): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")}, 2: {'output_adapter': Adapter(\n",
            "  (non_linearity): Activation_Function_Class(\n",
            "    (f): ReLU()\n",
            "  )\n",
            "  (adapter_down): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=48, bias=True)\n",
            "    (1): Activation_Function_Class(\n",
            "      (f): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")}, 3: {'output_adapter': Adapter(\n",
            "  (non_linearity): Activation_Function_Class(\n",
            "    (f): ReLU()\n",
            "  )\n",
            "  (adapter_down): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=48, bias=True)\n",
            "    (1): Activation_Function_Class(\n",
            "      (f): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")}, 4: {'output_adapter': Adapter(\n",
            "  (non_linearity): Activation_Function_Class(\n",
            "    (f): ReLU()\n",
            "  )\n",
            "  (adapter_down): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=48, bias=True)\n",
            "    (1): Activation_Function_Class(\n",
            "      (f): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")}}\n",
            "la_t config: {0: {'output_adapter': Adapter(\n",
            "  (non_linearity): Activation_Function_Class(\n",
            "    (f): ReLU()\n",
            "  )\n",
            "  (adapter_down): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=48, bias=True)\n",
            "    (1): Activation_Function_Class(\n",
            "      (f): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")}, 1: {'output_adapter': Adapter(\n",
            "  (non_linearity): Activation_Function_Class(\n",
            "    (f): ReLU()\n",
            "  )\n",
            "  (adapter_down): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=48, bias=True)\n",
            "    (1): Activation_Function_Class(\n",
            "      (f): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")}, 2: {'output_adapter': Adapter(\n",
            "  (non_linearity): Activation_Function_Class(\n",
            "    (f): ReLU()\n",
            "  )\n",
            "  (adapter_down): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=48, bias=True)\n",
            "    (1): Activation_Function_Class(\n",
            "      (f): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")}, 3: {'output_adapter': Adapter(\n",
            "  (non_linearity): Activation_Function_Class(\n",
            "    (f): ReLU()\n",
            "  )\n",
            "  (adapter_down): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=48, bias=True)\n",
            "    (1): Activation_Function_Class(\n",
            "      (f): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")}, 4: {'output_adapter': Adapter(\n",
            "  (non_linearity): Activation_Function_Class(\n",
            "    (f): ReLU()\n",
            "  )\n",
            "  (adapter_down): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=48, bias=True)\n",
            "    (1): Activation_Function_Class(\n",
            "      (f): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.active_adapters"
      ],
      "metadata": {
        "id": "ibPyICDzgQC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81f69918-7766-42c5-d35c-496ff6349d96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Stack[la_t]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# currently active adapters\n",
        "print(\"Active adapters:\", model.active_adapters)"
      ],
      "metadata": {
        "id": "r0SiG5QUg7Fq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92dc0051-0a93-4510-a709-03ea7e254ca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Active adapters: Stack[la_t]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SFE_MODEL = model"
      ],
      "metadata": {
        "id": "tGbQWhGEAIBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "def test_mlm_with_adapter(text, adapter_name=None):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].clone().to(model.device)\n",
        "    attention_mask = inputs[\"attention_mask\"].clone().to(model.device)\n",
        "\n",
        "    masked_indices = (input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
        "\n",
        "    if adapter_name:\n",
        "        model.set_active_adapters(Stack(adapter_name))\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "    # Move results back to CPU for token conversion\n",
        "    input_ids_cpu = input_ids.cpu()\n",
        "    for idx in masked_indices:\n",
        "        predicted_token_id = torch.argmax(outputs.logits[0, idx], dim=-1).item()\n",
        "        input_ids_cpu[0, idx] = predicted_token_id\n",
        "\n",
        "    original_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
        "    predicted_tokens = tokenizer.convert_ids_to_tokens(input_ids_cpu.squeeze().tolist())\n",
        "    original_text = tokenizer.convert_tokens_to_string(original_tokens)\n",
        "    predicted_text = tokenizer.convert_tokens_to_string(predicted_tokens)\n",
        "    return {\n",
        "        \"original_tokens\": original_tokens,\n",
        "        \"predicted_tokens\": predicted_tokens,\n",
        "        \"original_text\": original_text,\n",
        "        \"predicted_text\": predicted_text\n",
        "    }\n",
        "\n",
        "# Test\n",
        "test_text = \"ئۇيغۇر تىلى مېنىڭ مەدەنىي مىراثىم.\"\n",
        "result = test_mlm_with_adapter(test_text, \"la_s\")\n",
        "\n",
        "print(\"original_tokens:\", result[\"original_tokens\"])\n",
        "print(\"predicted_tokens:\", result[\"predicted_tokens\"])\n",
        "print(\"original_text:\", result[\"original_text\"])\n",
        "print(\"predicted_text:\", result[\"predicted_text\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgcyE8nqjI0k",
        "outputId": "cd6f12d5-68bb-4e14-9f54-b5ad23d66207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original_tokens: ['[CLS]', 'ئ', '##ۇ', '##ي', '##غ', '##ۇ', '##ر', 'ت', '##ى', '##لى', '[UNK]', '[UNK]', 'م', '##ى', '##را', '##ث', '##ى', '##م', '.', '[SEP]']\n",
            "predicted_tokens: ['[CLS]', 'ئ', '##ۇ', '##ي', '##غ', '##ۇ', '##ر', 'ت', '##ى', '##لى', '[UNK]', '[UNK]', 'م', '##ى', '##را', '##ث', '##ى', '##م', '.', '[SEP]']\n",
            "original_text: [CLS] ئۇيغۇر تىلى [UNK] [UNK] مىراثىم. [SEP]\n",
            "predicted_text: [CLS] ئۇيغۇر تىلى [UNK] [UNK] مىراثىم. [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AdapterFusionPLus"
      ],
      "metadata": {
        "id": "88e9EPDLo5Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model.to(device)\n",
        "\n",
        "class AdapterFusionPlus(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(2 * hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 2)\n",
        "        )\n",
        "    def forward(self, outputs_s, outputs_t):\n",
        "        combined = torch.cat([outputs_s, outputs_t], dim=-1)  # [B, T, 2*H]\n",
        "        raw_weights = self.mlp(combined)                      # [B, T, 2]\n",
        "        weights = torch.softmax(raw_weights, dim=-1)          # [B, T, 2]\n",
        "\n",
        "        w_s = weights[:, :, 0].unsqueeze(-1)  # [B, T, 1]\n",
        "        w_t = weights[:, :, 1].unsqueeze(-1)  # [B, T, 1]\n",
        "\n",
        "        fused_output = w_s * outputs_s + w_t * outputs_t      # [B, T, H]\n",
        "        return fused_output\n",
        "\n",
        "fusion_layer = AdapterFusionPlus(model.config.hidden_size)\n",
        "\n",
        "def forward_with_fusion(input_ids):\n",
        "    outputs_s = model(input_ids, adapter_names=\"la_s\")\n",
        "    outputs_t = model(input_ids, adapter_names=\"la_t\")\n",
        "    fused_output = fusion_layer(outputs_s.last_hidden_state, outputs_t.last_hidden_state)\n",
        "    return fused_output"
      ],
      "metadata": {
        "id": "uGhMKFxgZsCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## POS"
      ],
      "metadata": {
        "id": "JAAIa9wXX0Oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import re\n",
        "from transformers import AutoModelForTokenClassification, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn_crfsuite import CRF, metrics\n",
        "from conllu import parse_incr\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import defaultdict\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "def build_pos_vocab(conllu_data_path):\n",
        "    pos_vocab = defaultdict(int)\n",
        "\n",
        "    with open(conllu_data_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            if line.startswith(\"#\") or not line.strip():\n",
        "                continue\n",
        "\n",
        "            parts = line.strip().split(\"\\t\")\n",
        "\n",
        "            if \"-\" in parts[0]:\n",
        "                continue\n",
        "            pos_tag = parts[3]\n",
        "            pos_vocab[pos_tag] += 1\n",
        "\n",
        "    pos_vocab[\"[PAD]\"] = 0  # For padding\n",
        "    pos_vocab[\"[UNK]\"] = 0  # For unknown tags\n",
        "\n",
        "    return pos_vocab\n",
        "\n",
        "\n",
        "conllu_pth = '/content/ug_udt-ud-train.conllu'\n",
        "\n",
        "all_pos_tags = []\n",
        "with open(conllu_pth, \"r\") as f:\n",
        "    for line in f:\n",
        "        if line.startswith(\"#\") or not line.strip():\n",
        "            continue\n",
        "        parts = line.strip().split(\"\\t\")\n",
        "        if \"-\" in parts[0]:\n",
        "            continue\n",
        "        all_pos_tags.append(parts[3])  # or parts[4]\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(all_pos_tags + [\"[PAD]\", \"[UNK]\"])\n",
        "\n",
        "num_pos_tags = len(le.classes_)\n",
        "\n",
        "pos_vocab = build_pos_vocab(conllu_pth)\n",
        "num_pos_tags = len(pos_vocab)\n",
        "print(f\"Number of POS tags: {num_pos_tags}\")\n",
        "\n",
        "sorted_tags = sorted(\n",
        "    [tag for tag in pos_vocab.keys() if tag not in [\"[PAD]\", \"[UNK]\"]],\n",
        "    key=lambda x: pos_vocab[x],\n",
        "    reverse=True\n",
        ")\n",
        "\n",
        "sorted_tags += [\"[PAD]\", \"[UNK]\"]\n",
        "\n",
        "#  label2id mapping\n",
        "pos_label2id = {tag: idx for idx, tag in enumerate(sorted_tags)}\n",
        "id2pos_label = {idx: tag for tag, idx in pos_label2id.items()}\n",
        "\n",
        "print(pos_label2id)\n",
        "\n",
        "\n",
        "def load_conllu_data(file_path):\n",
        "    \"\"\"Loads CoNLL-U formatted data and extracts sentences with POS tags.\"\"\"\n",
        "    data_file = open(file_path, \"r\", encoding=\"utf-8\")\n",
        "    ud_treebank = []\n",
        "\n",
        "    for tokenlist in parse_incr(data_file):\n",
        "        tokens, tags = [], []\n",
        "        for token in tokenlist:\n",
        "            tokens.append(token[\"form\"])\n",
        "            tags.append(token[\"upostag\"])\n",
        "        ud_treebank.append((tokens, tags))\n",
        "\n",
        "    return ud_treebank\n",
        "\n",
        "conllu_pth = '/content/ug_udt-ud-train.conllu'\n",
        "ud_treebank = load_conllu_data(conllu_pth)\n",
        "\n",
        "\n",
        "def extract_features(sentence, index):\n",
        "    \"\"\"Extracts linguistic features from a given sentence at a specific index.\"\"\"\n",
        "    return {\n",
        "        'word': sentence[index],\n",
        "        'is_first': index == 0,\n",
        "        'is_last': index == len(sentence) - 1,\n",
        "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
        "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
        "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
        "        'is_alphanumeric': bool(re.match(r'^(?=.*[0-9]$)(?=.*[a-zA-Z])', sentence[index])),\n",
        "        'prefix-1': sentence[index][0],\n",
        "        'prefix-2': sentence[index][:2],\n",
        "        'prefix-3': sentence[index][:3],\n",
        "        'suffix-1': sentence[index][-1],\n",
        "        'suffix-2': sentence[index][-2:],\n",
        "        'suffix-3': sentence[index][-3:],\n",
        "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
        "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
        "        'has_hyphen': '-' in sentence[index],\n",
        "        'is_numeric': sentence[index].isdigit(),\n",
        "    }\n",
        "\n",
        "def transform_to_dataset(tagged_sentences):\n",
        "    \"\"\"Transforms tokenized sentences into feature sets and corresponding labels.\"\"\"\n",
        "    X, y = [], []\n",
        "    for sentence, tags in tagged_sentences:\n",
        "        X.append([extract_features(sentence, i) for i in range(len(sentence))])\n",
        "        y.append(tags)\n",
        "    return X, y\n"
      ],
      "metadata": {
        "id": "BJ0ATeGHX1Y3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bd6197e-052a-48b1-d0ec-9677632b114d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of POS tags: 12\n",
            "{'NOUN': 0, 'PUNCT': 1, 'VERB': 2, 'NUM': 3, 'PRON': 4, 'ADJ': 5, 'PROPN': 6, 'INTJ': 7, 'ADV': 8, 'AUX': 9, '[PAD]': 10, '[UNK]': 11}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "class POSDataset(Dataset):\n",
        "    \"\"\"Custom dataset for POS tagging with BERT tokenizer.\"\"\"\n",
        "    def __init__(self, ud_treebank, tokenizer, max_length=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.features = []\n",
        "        self.labels = []\n",
        "        self.sentences = []\n",
        "\n",
        "        all_tags = {tag for _, tags in ud_treebank for tag in tags}\n",
        "        self.label_map = {tag: i for i, tag in enumerate(sorted(all_tags))}\n",
        "        self.id2label = {i: tag for tag, i in self.label_map.items()}\n",
        "\n",
        "        for tokens, tags in ud_treebank:\n",
        "            self.sentences.append(tokens)\n",
        "            inputs = tokenizer(\n",
        "                tokens,\n",
        "                is_split_into_words=True,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=max_length,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            word_ids = inputs.word_ids()\n",
        "            previous_word_id = None\n",
        "            label_ids = []\n",
        "\n",
        "            for word_id in word_ids:\n",
        "                if word_id is None:\n",
        "                    label_ids.append(-100)\n",
        "                elif word_id != previous_word_id:\n",
        "                    label_ids.append(self.label_map[tags[word_id]])\n",
        "                else:\n",
        "                    label_ids.append(-100)\n",
        "                previous_word_id = word_id\n",
        "\n",
        "            self.features.append({k: v.squeeze(0) for k, v in inputs.items()})\n",
        "            self.labels.append(torch.tensor(label_ids))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        readable_labels = [self.id2label[label.item()] for label in self.labels[idx] if label.item() != -100]\n",
        "\n",
        "        return {\n",
        "            \"sentence\": sentence,\n",
        "            \"input_ids\": self.features[idx][\"input_ids\"],\n",
        "            \"attention_mask\": self.features[idx][\"attention_mask\"],\n",
        "            \"labels_readable\": readable_labels,\n",
        "            \"labels\": self.labels[idx]\n",
        "        }\n",
        "\n",
        "dataset = POSDataset(ud_treebank, tokenizer)\n",
        "\n",
        "sample = dataset[0]\n",
        "print(\"Sentence:\", \" \".join(sample[\"sentence\"]))\n",
        "print(\"Labels:\", sample[\"labels_readable\"])\n"
      ],
      "metadata": {
        "id": "rSa9kPUMX28l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a1164c3-92db-4a58-acd7-c6ccb3af0067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: نەشپۈت بەش يىلدا ، ئۆرۈك تۆت يىلدا مېۋە بېرىدۇ دېگەننى ئاڭلىمىغانمىدىڭ ؟\n",
            "Labels: ['NOUN', 'NUM', 'NOUN', 'PUNCT', 'NOUN', 'NUM', 'NOUN', 'NOUN', 'VERB', 'VERB', 'VERB', 'PUNCT']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS USING AdapterFusionPlus"
      ],
      "metadata": {
        "id": "VkT30c1oYGFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class POSClassifier(nn.Module):\n",
        "    \"\"\"Final classifier for POS tagging.\"\"\"\n",
        "    def __init__(self, hidden_size, num_labels):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, fused_output):\n",
        "        return self.classifier(fused_output)\n",
        "\n",
        "class POSFusionModel(nn.Module):\n",
        "    \"\"\"POS tagging model with AdapterFusion+.\"\"\"\n",
        "    def __init__(self, base_model, fusion_layer, classifier):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.fusion_layer = fusion_layer\n",
        "        self.classifier = classifier\n",
        "        self.base_model.config.output_hidden_states = True\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs_s = self.base_model(input_ids=input_ids, attention_mask=attention_mask, adapter_names=\"la_s\")\n",
        "        outputs_t = self.base_model(input_ids=input_ids, attention_mask=attention_mask, adapter_names=\"la_t\")\n",
        "        fused = self.fusion_layer(outputs_s.hidden_states[-1], outputs_t.hidden_states[-1])\n",
        "        return self.classifier(fused)\n",
        "\n",
        "\n",
        "def train_pos_model(model, dataloader, num_epochs=3, lr=1e-4):\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids, attention_mask, labels = batch[\"input_ids\"].cuda(), batch[\"attention_mask\"].cuda(), batch[\"labels\"].cuda()\n",
        "\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            loss = criterion(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {total_loss / len(dataloader):.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_pos_model(model, dataloader):\n",
        "    model.eval()\n",
        "    total_correct, total_tokens = 0, 0\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids, attention_mask, labels = (\n",
        "                batch[\"input_ids\"].cuda(),\n",
        "                batch[\"attention_mask\"].cuda(),\n",
        "                batch[\"labels\"].cuda(),\n",
        "            )\n",
        "\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            # print(\"labels\",labels )\n",
        "            # print(\"predictions\", predictions)\n",
        "            mask = labels != -100\n",
        "            correct = (predictions == labels) & mask\n",
        "            total_correct += correct.sum().item()\n",
        "            total_tokens += mask.sum().item()\n",
        "            y_true.extend(labels[mask].cpu().numpy().tolist())\n",
        "            y_pred.extend(predictions[mask].cpu().numpy().tolist())\n",
        "\n",
        "    print(f\"POS Tagging Accuracy: {total_correct / total_tokens:.4f}\")\n",
        "    return y_true, y_pred\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = [item[\"input_ids\"] for item in batch]\n",
        "    attention_mask = [item[\"attention_mask\"] for item in batch]\n",
        "    labels = [item[\"labels\"] for item in batch]\n",
        "\n",
        "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
        "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
        "\n",
        "    return {\"input_ids\": input_ids.cuda(), \"attention_mask\": attention_mask.cuda(), \"labels\": labels.cuda()}\n",
        "\n",
        "train_dataset = POSDataset(load_conllu_data(conllu_pth), tokenizer)\n",
        "test_dataset = POSDataset(load_conllu_data(\"/content/ug_udt-ud-dev.conllu\"), tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "fusion_layer = AdapterFusionPlus(768).cuda()\n",
        "pos_classifier = POSClassifier(768, len(train_dataset.label_map)).cuda()\n",
        "model1 =  SFE_MODEL\n",
        "pos_model = POSFusionModel(model1, fusion_layer, pos_classifier).cuda()\n",
        "\n",
        "train_pos_model(pos_model, train_loader, num_epochs=100)\n",
        "y_true, y_pred = evaluate_pos_model(pos_model, test_loader)"
      ],
      "metadata": {
        "id": "54cc435VX5Ta",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420242c8-9532-4460-fe2f-26f090ef004b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Loss: 2.3731\n",
            "Epoch 2/100 - Loss: 2.3246\n",
            "Epoch 3/100 - Loss: 2.2854\n",
            "Epoch 4/100 - Loss: 2.2377\n",
            "Epoch 5/100 - Loss: 2.2047\n",
            "Epoch 6/100 - Loss: 2.1608\n",
            "Epoch 7/100 - Loss: 2.1269\n",
            "Epoch 8/100 - Loss: 2.0858\n",
            "Epoch 9/100 - Loss: 2.0492\n",
            "Epoch 10/100 - Loss: 2.0222\n",
            "Epoch 11/100 - Loss: 1.9909\n",
            "Epoch 12/100 - Loss: 1.9627\n",
            "Epoch 13/100 - Loss: 1.9421\n",
            "Epoch 14/100 - Loss: 1.9120\n",
            "Epoch 15/100 - Loss: 1.8956\n",
            "Epoch 16/100 - Loss: 1.8666\n",
            "Epoch 17/100 - Loss: 1.8506\n",
            "Epoch 18/100 - Loss: 1.8291\n",
            "Epoch 19/100 - Loss: 1.8015\n",
            "Epoch 20/100 - Loss: 1.7922\n",
            "Epoch 21/100 - Loss: 1.7759\n",
            "Epoch 22/100 - Loss: 1.7619\n",
            "Epoch 23/100 - Loss: 1.7509\n",
            "Epoch 24/100 - Loss: 1.7345\n",
            "Epoch 25/100 - Loss: 1.7185\n",
            "Epoch 26/100 - Loss: 1.7060\n",
            "Epoch 27/100 - Loss: 1.6931\n",
            "Epoch 28/100 - Loss: 1.6952\n",
            "Epoch 29/100 - Loss: 1.6605\n",
            "Epoch 30/100 - Loss: 1.6669\n",
            "Epoch 31/100 - Loss: 1.6470\n",
            "Epoch 32/100 - Loss: 1.6418\n",
            "Epoch 33/100 - Loss: 1.6342\n",
            "Epoch 34/100 - Loss: 1.6245\n",
            "Epoch 35/100 - Loss: 1.6138\n",
            "Epoch 36/100 - Loss: 1.5991\n",
            "Epoch 37/100 - Loss: 1.5914\n",
            "Epoch 38/100 - Loss: 1.5914\n",
            "Epoch 39/100 - Loss: 1.5816\n",
            "Epoch 40/100 - Loss: 1.5821\n",
            "Epoch 41/100 - Loss: 1.5691\n",
            "Epoch 42/100 - Loss: 1.5491\n",
            "Epoch 43/100 - Loss: 1.5263\n",
            "Epoch 44/100 - Loss: 1.5423\n",
            "Epoch 45/100 - Loss: 1.5199\n",
            "Epoch 46/100 - Loss: 1.5318\n",
            "Epoch 47/100 - Loss: 1.5192\n",
            "Epoch 48/100 - Loss: 1.5074\n",
            "Epoch 49/100 - Loss: 1.5071\n",
            "Epoch 50/100 - Loss: 1.4856\n",
            "Epoch 51/100 - Loss: 1.4570\n",
            "Epoch 52/100 - Loss: 1.4854\n",
            "Epoch 53/100 - Loss: 1.4718\n",
            "Epoch 54/100 - Loss: 1.4711\n",
            "Epoch 55/100 - Loss: 1.4448\n",
            "Epoch 56/100 - Loss: 1.4495\n",
            "Epoch 57/100 - Loss: 1.4438\n",
            "Epoch 58/100 - Loss: 1.4359\n",
            "Epoch 59/100 - Loss: 1.4401\n",
            "Epoch 60/100 - Loss: 1.4383\n",
            "Epoch 61/100 - Loss: 1.4084\n",
            "Epoch 62/100 - Loss: 1.3937\n",
            "Epoch 63/100 - Loss: 1.4067\n",
            "Epoch 64/100 - Loss: 1.3903\n",
            "Epoch 65/100 - Loss: 1.3955\n",
            "Epoch 66/100 - Loss: 1.3858\n",
            "Epoch 67/100 - Loss: 1.3744\n",
            "Epoch 68/100 - Loss: 1.3755\n",
            "Epoch 69/100 - Loss: 1.3686\n",
            "Epoch 70/100 - Loss: 1.3647\n",
            "Epoch 71/100 - Loss: 1.3567\n",
            "Epoch 72/100 - Loss: 1.3559\n",
            "Epoch 73/100 - Loss: 1.3596\n",
            "Epoch 74/100 - Loss: 1.3296\n",
            "Epoch 75/100 - Loss: 1.3532\n",
            "Epoch 76/100 - Loss: 1.3304\n",
            "Epoch 77/100 - Loss: 1.3249\n",
            "Epoch 78/100 - Loss: 1.3051\n",
            "Epoch 79/100 - Loss: 1.2972\n",
            "Epoch 80/100 - Loss: 1.3094\n",
            "Epoch 81/100 - Loss: 1.3106\n",
            "Epoch 82/100 - Loss: 1.3030\n",
            "Epoch 83/100 - Loss: 1.2911\n",
            "Epoch 84/100 - Loss: 1.2848\n",
            "Epoch 85/100 - Loss: 1.2788\n",
            "Epoch 86/100 - Loss: 1.2756\n",
            "Epoch 87/100 - Loss: 1.2654\n",
            "Epoch 88/100 - Loss: 1.2852\n",
            "Epoch 89/100 - Loss: 1.2658\n",
            "Epoch 90/100 - Loss: 1.2520\n",
            "Epoch 91/100 - Loss: 1.2637\n",
            "Epoch 92/100 - Loss: 1.2598\n",
            "Epoch 93/100 - Loss: 1.2462\n",
            "Epoch 94/100 - Loss: 1.2453\n",
            "Epoch 95/100 - Loss: 1.2307\n",
            "Epoch 96/100 - Loss: 1.2177\n",
            "Epoch 97/100 - Loss: 1.2186\n",
            "Epoch 98/100 - Loss: 1.2139\n",
            "Epoch 99/100 - Loss: 1.2167\n",
            "Epoch 100/100 - Loss: 1.2105\n",
            "POS Tagging Accuracy: 0.0968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_pos(sentence):\n",
        "    inputs = tokenizer(sentence.split(), is_split_into_words=True, padding=\"max_length\", truncation=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = pos_model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
        "\n",
        "    predictions = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
        "    pred_labels = [id2pos_label[i] for i in predictions if i != -100]\n",
        "\n",
        "    return list(zip(sentence.split(), pred_labels))\n",
        "\n",
        "\n",
        "test_sentence = \"نەشپۈت بەش يىلدا، ئۆرۈك تۆت يىلدا مېۋە بېرىدۇ دېگەننى ئاڭلىمىغانمىدىڭ؟\"\n",
        "# test_sentence = \"neshpüt besh yilda, örük töt yilda mëwe bëridu dëgenni anglimighanmiding?\"\n",
        "print(predict_pos(test_sentence))"
      ],
      "metadata": {
        "id": "yzrUcdLbX_ww",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2217412f-7dca-4537-d74a-71f8e2aa27a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('نەشپۈت', 'ADV'), ('بەش', 'PRON'), ('يىلدا،', 'PRON'), ('ئۆرۈك', 'PRON'), ('تۆت', 'PRON'), ('يىلدا', 'PRON'), ('مېۋە', 'ADV'), ('بېرىدۇ', 'ADV'), ('دېگەننى', 'PRON'), ('ئاڭلىمىغانمىدىڭ؟', 'ADJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "print(\"## AdapterFusion Model Evaluation ##\")\n",
        "\n",
        "f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "print(\"F1 score on Dev Data:\", f1)\n",
        "\n",
        "print(\"\\nClass-wise scores:\")\n",
        "print(classification_report(y_true, y_pred, target_names=test_dataset.label_map.keys()))"
      ],
      "metadata": {
        "id": "5bIGaV5KYCd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfd96655-cee1-4bc5-a5e8-5aaad1248c54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## AdapterFusion Model Evaluation ##\n",
            "F1 score on Dev Data: 0.08887595610242767\n",
            "\n",
            "Class-wise scores:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.00      0.00      0.00        16\n",
            "         ADP       0.00      0.00      0.00         4\n",
            "         ADV       0.00      0.00      0.00         4\n",
            "         AUX       0.00      0.00      0.00         7\n",
            "       CCONJ       0.01      1.00      0.02         1\n",
            "        NOUN       0.00      0.00      0.00        36\n",
            "         NUM       0.00      0.00      0.00         8\n",
            "        PRON       0.00      0.00      0.00        18\n",
            "       PUNCT       1.00      1.00      1.00        11\n",
            "       SCONJ       0.00      0.00      0.00         1\n",
            "        VERB       0.00      0.00      0.00        18\n",
            "\n",
            "    accuracy                           0.10       124\n",
            "   macro avg       0.09      0.18      0.09       124\n",
            "weighted avg       0.09      0.10      0.09       124\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## POS USING AdapterFusion"
      ],
      "metadata": {
        "id": "RjfnP0X7YMVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from adapters.composition import Fuse\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "\n",
        "train_data = load_conllu_data(conllu_pth)\n",
        "dev_data = load_conllu_data(\"/content/ug_udt-ud-test.conllu\")\n",
        "\n",
        "# POS label mapping\n",
        "all_tags = sorted({tag for _, tags in train_data+dev_data for tag in tags})\n",
        "id2label = {i: tag for i, tag in enumerate(all_tags)}\n",
        "label2id = {tag: i for i, tag in id2label.items()}\n",
        "\n",
        "# Tokenization and Dataset\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "train_dataset = POSDataset(train_data, tokenizer)\n",
        "dev_dataset = POSDataset(dev_data, tokenizer)\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions = np.argmax(p.predictions, axis=2)\n",
        "    true_labels = p.label_ids\n",
        "    mask = true_labels != -100\n",
        "    y_true = true_labels[mask].flatten()\n",
        "    y_pred = predictions[mask].flatten()\n",
        "\n",
        "    return {\n",
        "        \"f1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred)\n",
        "    }"
      ],
      "metadata": {
        "id": "eGpIKxQbYPdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "model = AutoAdapterModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "# Load two adapters\n",
        "model.load_adapter(\"la_s\", load_as=\"source_script\")\n",
        "model.load_adapter(\"la_t\", load_as=\"transliterated\")\n",
        "\n",
        "adapter_setup = Fuse(\"source_script\", \"transliterated\")\n",
        "model.add_adapter_fusion(adapter_setup)\n",
        "\n",
        "# POS classification head\n",
        "model.add_tagging_head(\"pos_head\", num_labels=len(id2label))\n",
        "\n",
        "# Activate fusion\n",
        "model.train_adapter_fusion(adapter_setup)\n",
        "\n",
        "#  Training\n",
        "training_args = TrainingArguments(\n",
        "    learning_rate=1e-4,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=1,\n",
        "    logging_dir=\"./logs\",\n",
        "    output_dir=\"./pos_fusion\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "trainer = AdapterTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=dev_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "xZzDGBShYR_j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "762d800d-7406-454a-cc7c-70466f1fe301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertAdapterModel were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['heads.default.3.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [80/80 01:23, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.664313</td>\n",
              "      <td>0.013478</td>\n",
              "      <td>0.030172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.826080</td>\n",
              "      <td>0.012737</td>\n",
              "      <td>0.030172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.952394</td>\n",
              "      <td>0.004940</td>\n",
              "      <td>0.021552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.037903</td>\n",
              "      <td>0.000771</td>\n",
              "      <td>0.017241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.065980</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.017241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.044214</td>\n",
              "      <td>0.000884</td>\n",
              "      <td>0.017241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.053801</td>\n",
              "      <td>0.000907</td>\n",
              "      <td>0.017241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.073239</td>\n",
              "      <td>0.000867</td>\n",
              "      <td>0.017241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.087412</td>\n",
              "      <td>0.000857</td>\n",
              "      <td>0.017241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.088597</td>\n",
              "      <td>0.000867</td>\n",
              "      <td>0.017241</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=80, training_loss=1.6398483276367188, metrics={'train_runtime': 83.2067, 'train_samples_per_second': 1.923, 'train_steps_per_second': 0.961, 'total_flos': 13425909104640.0, 'train_loss': 1.6398483276367188, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation and Prediction\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def predict_pos(sentence):\n",
        "    tokenized = tokenizer(\n",
        "        sentence.split(),\n",
        "        is_split_into_words=True,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokenized)\n",
        "\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1).squeeze().tolist()\n",
        "    word_ids = tokenized.word_ids(batch_index=0)\n",
        "\n",
        "    pred_tags = []\n",
        "    current_word = None\n",
        "    for i, word_idx in enumerate(word_ids):\n",
        "        if word_idx != current_word and word_idx is not None:\n",
        "            pred_tags.append(id2label[predictions[i]])\n",
        "            current_word = word_idx\n",
        "\n",
        "    return list(zip(sentence.split(), pred_tags))\n",
        "\n",
        "# test_sentence = \"نەشپۈت بەش يىلدا، ئۆرۈك تۆت يىلدا مېۋە بېرىدۇ دېگەننى ئاڭلىمىغانمىدىڭ؟\"\n",
        "# test_sentence = \"neshpüt besh yilda, örük töt yilda mëwe bëridu dëgenni anglimighanmiding ?\"\n",
        "test_sentence = \"سەن شۇ چاققىچە تاقەت قىلىپ تۇرالامسەن!\"\n",
        "print(predict_pos(test_sentence))\n",
        "\n",
        "# Evaluation Report\n",
        "def get_true_pred(model, dataset):\n",
        "    all_true = []\n",
        "    all_pred = []\n",
        "    model.eval()\n",
        "\n",
        "    for item in dataset:\n",
        "        inputs = {\n",
        "            \"input_ids\": item[\"input_ids\"].unsqueeze(0).to(device),\n",
        "            \"attention_mask\": item[\"attention_mask\"].unsqueeze(0).to(device)\n",
        "        }\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        preds = torch.argmax(outputs.logits, dim=-1).squeeze().cpu().tolist()\n",
        "        labels = item[\"labels\"].tolist()\n",
        "\n",
        "        # mask ignoring -100 labels\n",
        "        mask = [label != -100 for label in labels]\n",
        "        all_true.extend([labels[i] for i, m in enumerate(mask) if m])\n",
        "        all_pred.extend([preds[i] for i, m in enumerate(mask) if m])\n",
        "\n",
        "    return all_true, all_pred\n",
        "\n",
        "# predictions and labels\n",
        "y_true, y_pred = get_true_pred(model, dev_dataset)\n",
        "\n",
        "# classification report\n",
        "print(\"## AdapterFusion Model Evaluation ##\")\n",
        "print(\"F1 score on Dev Data:\", f1_score(y_true, y_pred, average=\"weighted\"))\n",
        "print(\"\\nClass-wise scores:\")\n",
        "print(classification_report(\n",
        "    [id2label[t] for t in y_true],\n",
        "    [id2label[p] for p in y_pred],\n",
        "    digits=3,\n",
        "    zero_division=0\n",
        "))"
      ],
      "metadata": {
        "id": "N1PNAL0vYfRW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f56f21e8-2e01-4001-e1f3-2743d076edc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('سەن', 'CCONJ'), ('شۇ', 'CCONJ'), ('چاققىچە', 'CCONJ'), ('تاقەت', 'CCONJ'), ('قىلىپ', 'PRON'), ('تۇرالامسەن!', 'PRON')]\n",
            "## AdapterFusion Model Evaluation ##\n",
            "F1 score on Dev Data: 0.0008674907829104316\n",
            "\n",
            "Class-wise scores:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ      0.000     0.000     0.000        15\n",
            "         ADP      0.000     0.000     0.000         9\n",
            "         ADV      0.000     0.000     0.000         7\n",
            "         AUX      0.000     0.000     0.000         7\n",
            "       CCONJ      0.026     1.000     0.050         4\n",
            "         DET      0.000     0.000     0.000         2\n",
            "        INTJ      0.000     0.000     0.000        88\n",
            "        NOUN      0.000     0.000     0.000         4\n",
            "         NUM      0.000     0.000     0.000        12\n",
            "        PRON      0.000     0.000     0.000        39\n",
            "       PROPN      0.000     0.000     0.000        45\n",
            "\n",
            "    accuracy                          0.017       232\n",
            "   macro avg      0.002     0.091     0.005       232\n",
            "weighted avg      0.000     0.017     0.001       232\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependency Parsing AdapterFusionPlus"
      ],
      "metadata": {
        "id": "rbMcrRnXJ0YU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import AutoTokenizer, AdamW\n",
        "from adapters import AutoAdapterModel\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class AdapterFusionPlus(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(2 * hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 2)\n",
        "        )\n",
        "    def forward(self, outputs_s, outputs_t):\n",
        "        combined = torch.cat([outputs_s, outputs_t], dim=-1)   # [B, T, 2*H]\n",
        "        raw_weights = self.mlp(combined)                       # [B, T, 2]\n",
        "        weights = torch.softmax(raw_weights, dim=-1)           # [B, T, 2]\n",
        "        w_s = weights[:, :, 0].unsqueeze(-1)                   # [B, T, 1]\n",
        "        w_t = weights[:, :, 1].unsqueeze(-1)                   # [B, T, 1]\n",
        "        fused_output = w_s * outputs_s + w_t * outputs_t       # [B, T, H]\n",
        "        return fused_output\n",
        "\n",
        "def create_dep_label_mapping(file_path):\n",
        "    \"\"\"\n",
        "    Reads a CoNLL-U file and generates a unique mapping for dependency labels.\n",
        "    \"\"\"\n",
        "    dep_labels_set = set()\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if not line.startswith(\"#\") and line.strip():\n",
        "                parts = line.split(\"\\t\")\n",
        "                if len(parts) == 10:\n",
        "                    dep_labels_set.add(parts[7])\n",
        "    dep_label2id = {label: idx for idx, label in enumerate(sorted(dep_labels_set))}\n",
        "    dep_id2label = {idx: label for label, idx in dep_label2id.items()}\n",
        "    return dep_label2id, dep_id2label\n",
        "\n",
        "conllu_pth = \"ug_udt-ud-train.conllu\"\n",
        "file_path = conllu_pth\n",
        "dep_label2id, dep_id2label = create_dep_label_mapping(file_path)\n",
        "NUM_DEP_LABELS = len(dep_label2id)\n",
        "print(f\"Dependency Labels Mapping: {dep_label2id}\")\n"
      ],
      "metadata": {
        "id": "qTNourwlymwG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "485952d9-aaf2-4ce1-ea28-b052f3e92a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependency Labels Mapping: {'acl': 0, 'advcl': 1, 'advmod': 2, 'amod': 3, 'ccomp': 4, 'compound': 5, 'conj': 6, 'cop': 7, 'det': 8, 'discourse': 9, 'dislocated': 10, 'flat': 11, 'nmod': 12, 'nsubj': 13, 'nummod': 14, 'obj': 15, 'obl': 16, 'orphan': 17, 'parataxis': 18, 'punct': 19, 'root': 20, 'vocative': 21, 'xcomp': 22}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ConlluDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, dep_label2id, max_length=128):\n",
        "        self.sentences = self.load_conllu(file_path, dep_label2id)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def load_conllu(self, file_path, dep_label2id):\n",
        "\n",
        "        sentences = []\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            sentence = []\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line == \"\":\n",
        "                    if sentence:\n",
        "                        sentences.append(sentence)\n",
        "                        sentence = []\n",
        "                elif not line.startswith(\"#\"):\n",
        "                    parts = line.split(\"\\t\")\n",
        "                    if len(parts) == 10:\n",
        "                        token_id, token, lemma, upos, xpos, feats, head, dep_rel, deps, misc = parts\n",
        "                        label = dep_label2id.get(dep_rel, -1)\n",
        "                        sentence.append((token, upos, int(head), label))\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "        return sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        tokens = [word[0] for word in sentence]\n",
        "        dep_heads = [word[2] for word in sentence]\n",
        "        dep_labels = [word[3] for word in sentence]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            tokens,\n",
        "            is_split_into_words=True,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=self.max_length,\n",
        "            return_offsets_mapping=True\n",
        "        )\n",
        "        word_starts = torch.zeros(len(encoding[\"input_ids\"][0]), dtype=torch.long)\n",
        "        word_ids = encoding.word_ids()\n",
        "        for i, word_idx in enumerate(word_ids):\n",
        "            if word_idx is not None and (i == 0 or word_ids[i-1] != word_idx):\n",
        "                word_starts[i] = 1\n",
        "\n",
        "        padded_dep_labels = torch.full((self.max_length,), -1, dtype=torch.long)\n",
        "        padded_dep_heads = torch.full((self.max_length,), -1, dtype=torch.long)\n",
        "        seq_len = len(dep_labels)\n",
        "        padded_dep_labels[:seq_len] = torch.tensor(dep_labels, dtype=torch.long)\n",
        "        padded_dep_heads[:seq_len] = torch.tensor(dep_heads, dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"dep_heads\": padded_dep_heads,\n",
        "            \"dep_labels\": padded_dep_labels,\n",
        "            \"word_starts\": word_starts       # shape: [max_length]\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
        "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
        "    dep_heads = torch.stack([item['dep_heads'] for item in batch])\n",
        "    dep_labels = torch.stack([item['dep_labels'] for item in batch])\n",
        "    word_starts = torch.stack([item['word_starts'] for item in batch])\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"dep_heads\": dep_heads,\n",
        "        \"dep_labels\": dep_labels,\n",
        "        \"word_starts\": word_starts\n",
        "    }\n"
      ],
      "metadata": {
        "id": "ANV9ON6vclOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "base_model = AutoAdapterModel.from_pretrained(\"bert-base-multilingual-cased\").to(device)\n",
        "base_model.load_adapter(\"la_s\")\n",
        "base_model.load_adapter(\"la_t\")\n",
        "\n",
        "class DependencyParser(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, num_labels):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "    def forward(self, fused_output):\n",
        "        return self.classifier(fused_output)\n",
        "\n",
        "class DependencyParserFusionModel(nn.Module):\n",
        "\n",
        "    def __init__(self, base_model, fusion_layer, classifier):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.fusion_layer = fusion_layer\n",
        "        self.classifier = classifier\n",
        "        self.base_model.config.output_hidden_states = True\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Activate and get outputs from the first adapter\n",
        "        self.base_model.set_active_adapters([\"la_s\"])\n",
        "        outputs_s = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Activate and get outputs from the second adapter\n",
        "        self.base_model.set_active_adapters([\"la_t\"])\n",
        "        outputs_t = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Fuse the outputs from both adapters\n",
        "        fused = self.fusion_layer(outputs_s.hidden_states[-1], outputs_t.hidden_states[-1])\n",
        "        # classification logits: shape [B, seq_len, NUM_DEP_LABELS]\n",
        "        return self.classifier(fused)\n",
        "\n",
        "fusion_layer = AdapterFusionPlus(base_model.config.hidden_size).to(device)\n",
        "dependency_parser_classifier = DependencyParser(base_model.config.hidden_size, NUM_DEP_LABELS).to(device)\n",
        "DepPar_model = DependencyParserFusionModel(base_model, fusion_layer, dependency_parser_classifier).to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue7dGTb89Svl",
        "outputId": "7ff21eb4-0323-4342-d0e3-de7809712f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertAdapterModel were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['heads.default.3.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_DepPar_model(model, dataloader, num_epochs=3, lr=1e-4):\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"dep_labels\"].to(device)\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            loss = criterion(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {total_loss / len(dataloader):.4f}\")\n",
        "\n",
        "def evaluate_DepPar_model(model, dataloader):\n",
        "    model.eval()\n",
        "    total_correct, total_tokens = 0, 0\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"dep_labels\"].to(device)\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            mask = labels != -1\n",
        "            correct = (predictions == labels) & mask\n",
        "            total_correct += correct.sum().item()\n",
        "            total_tokens += mask.sum().item()\n",
        "            y_true.extend(labels[mask].cpu().tolist())\n",
        "            y_pred.extend(predictions[mask].cpu().tolist())\n",
        "    las = total_correct / total_tokens if total_tokens > 0 else 0\n",
        "    print(f\"Dependency Parser LAS: {las:.4f}\")\n",
        "    return y_true, y_pred\n",
        "\n",
        "train_dataset = ConlluDataset(conllu_pth, tokenizer, dep_label2id)\n",
        "val_dataset = ConlluDataset(\"/content/ug_udt-ud-test.conllu\", tokenizer, dep_label2id)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "train_DepPar_model(DepPar_model, train_loader, num_epochs=10)\n",
        "_ = evaluate_DepPar_model(DepPar_model, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGscJQcMpaJx",
        "outputId": "454222a8-b96f-4b05-f225-46dd7f522f02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/adapters/composition.py:225: FutureWarning: Passing list objects for adapter activation is deprecated. Please use Stack or Fuse explicitly.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Loss: 2.8163\n",
            "Epoch 2/10 - Loss: 2.7207\n",
            "Epoch 3/10 - Loss: 2.6777\n",
            "Epoch 4/10 - Loss: 2.6333\n",
            "Epoch 5/10 - Loss: 2.6665\n",
            "Epoch 6/10 - Loss: 2.6523\n",
            "Epoch 7/10 - Loss: 2.6224\n",
            "Epoch 8/10 - Loss: 2.6460\n",
            "Epoch 9/10 - Loss: 2.6841\n",
            "Epoch 10/10 - Loss: 2.6443\n",
            "Dependency Parser LAS: 0.2074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_sentence_dep_fusion(model, tokenizer, sentence, dep_id2label, max_length=128):\n",
        "\n",
        "    original_tokens = sentence.strip().split()\n",
        "\n",
        "    encoding = tokenizer(\n",
        "        original_tokens,\n",
        "        is_split_into_words=True,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=max_length,\n",
        "        return_offsets_mapping=True\n",
        "    )\n",
        "\n",
        "    word_ids = encoding.word_ids(0)\n",
        "    word_starts = torch.zeros(len(encoding[\"input_ids\"][0]), dtype=torch.long)\n",
        "    model_words = []\n",
        "    previous_word_idx = None\n",
        "    for pos, word_idx in enumerate(word_ids):\n",
        "        if word_idx is None:\n",
        "            continue\n",
        "        if word_idx != previous_word_idx:\n",
        "            word_starts[pos] = 1\n",
        "            model_words.append(original_tokens[word_idx])\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "    word_starts = word_starts.unsqueeze(0)\n",
        "    input_ids = encoding[\"input_ids\"].to(device)\n",
        "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids, attention_mask)  # shape: [1, seq_len, NUM_DEP_LABELS]\n",
        "        predictions = torch.argmax(logits, dim=-1).squeeze(0).cpu().tolist()\n",
        "\n",
        "    word_pred_labels = []\n",
        "    for pos, word_idx in enumerate(word_ids):\n",
        "        if word_idx is None:\n",
        "            continue\n",
        "        if pos == 0 or (word_ids[pos - 1] != word_idx):\n",
        "            word_pred_labels.append(predictions[pos])\n",
        "\n",
        "    simulated_heads = []\n",
        "    for i in range(len(model_words)):\n",
        "        if i == 0:\n",
        "            simulated_heads.append(\"ROOT\")\n",
        "        else:\n",
        "            simulated_heads.append(model_words[i - 1])\n",
        "\n",
        "    print(\"Dependency Parsing Result:\")\n",
        "    print(\"OrigToken\\tParsedToken\\tPredicted Head\\tDependency Label\")\n",
        "    for i in range(len(model_words)):\n",
        "        token = original_tokens[i] if i < len(original_tokens) else \"UNK\"\n",
        "        parsed_token = model_words[i]\n",
        "        pred_label = dep_id2label.get(word_pred_labels[i], \"UNKNOWN\")\n",
        "        pred_head = simulated_heads[i]\n",
        "        print(f\"{token}\\t\\t{parsed_token}\\t\\t{pred_head}\\t\\t\\t{pred_label}\")\n",
        "\n",
        "test_sentence = \"نەشپۈت بەش يىلدا، ئۆرۈك تۆت يىلدا مېۋە بېرىدۇ دېگەننى ئاڭلىمىغانمىدىڭ ؟\"\n",
        "parse_sentence_dep_fusion(DepPar_model, tokenizer, test_sentence, dep_id2label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zbWCxCyU58E",
        "outputId": "a4df78af-e7f7-49bf-96b0-18a4ae4a6f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependency Parsing Result:\n",
            "OrigToken\tParsedToken\tPredicted Head\tDependency Label\n",
            "نەشپۈت\t\tنەشپۈت\t\tROOT\t\t\tpunct\n",
            "بەش\t\tبەش\t\tنەشپۈت\t\t\tpunct\n",
            "يىلدا،\t\tيىلدا،\t\tبەش\t\t\tpunct\n",
            "ئۆرۈك\t\tئۆرۈك\t\tيىلدا،\t\t\tpunct\n",
            "تۆت\t\tتۆت\t\tئۆرۈك\t\t\tpunct\n",
            "يىلدا\t\tيىلدا\t\tتۆت\t\t\tpunct\n",
            "مېۋە\t\tمېۋە\t\tيىلدا\t\t\tpunct\n",
            "بېرىدۇ\t\tبېرىدۇ\t\tمېۋە\t\t\tpunct\n",
            "دېگەننى\t\tدېگەننى\t\tبېرىدۇ\t\t\tpunct\n",
            "ئاڭلىمىغانمىدىڭ\t\tئاڭلىمىغانمىدىڭ\t\tدېگەننى\t\t\tpunct\n",
            "؟\t\t؟\t\tئاڭلىمىغانمىدىڭ\t\t\tpunct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependency Parsing AdapterFusion\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "65wSNa6py0Xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from adapters.composition import Fuse\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def create_dep_label_mapping(file_path):\n",
        "    mapping = {\n",
        "        'acl': 0, 'advcl': 1, 'advmod': 2, 'amod': 3, 'ccomp': 4,\n",
        "        'compound': 5, 'conj': 6, 'cop': 7, 'det': 8, 'discourse': 9,\n",
        "        'dislocated': 10, 'flat': 11, 'nmod': 12, 'nsubj': 13, 'nummod': 14,\n",
        "        'obj': 15, 'obl': 16, 'orphan': 17, 'parataxis': 18, 'punct': 19,\n",
        "        'root': 20, 'vocative': 21, 'xcomp': 22\n",
        "    }\n",
        "    id2label = {v: k for k, v in mapping.items()}\n",
        "    return mapping, id2label\n",
        "\n",
        "def load_conllu_data(path, head_offset=64):\n",
        "    data = []\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        lines = f.read().strip().split(\"\\n\")\n",
        "    sentence_lines = []\n",
        "    for line in lines:\n",
        "        if line.strip() == \"\":\n",
        "            if sentence_lines:\n",
        "                data.append(parse_sentence(sentence_lines, head_offset))\n",
        "                sentence_lines = []\n",
        "        else:\n",
        "            sentence_lines.append(line)\n",
        "    if sentence_lines:\n",
        "        data.append(parse_sentence(sentence_lines, head_offset))\n",
        "    return data\n",
        "\n",
        "def parse_sentence(lines, head_offset):\n",
        "    sent_text = None\n",
        "    tokens = []\n",
        "    dep_heads = []\n",
        "    dep_labels = []\n",
        "\n",
        "    for line in lines:\n",
        "        if line.startswith(\"# text =\"):\n",
        "            sent_text = line[len(\"# text =\"):].strip()\n",
        "            break\n",
        "    for line in lines:\n",
        "        if line.startswith(\"#\"):\n",
        "            continue\n",
        "        parts = line.split(\"\\t\")\n",
        "        if len(parts) < 8:\n",
        "            continue\n",
        "        if \"-\" in parts[0]:\n",
        "            continue\n",
        "        tokens.append(parts[1])\n",
        "        head = int(parts[6])\n",
        "        dep_heads.append(head)\n",
        "        dep_labels.append(parts[7])\n",
        "    if sent_text is None:\n",
        "        sent_text = \" \".join(tokens)\n",
        "    return {\n",
        "        \"sentence\": sent_text,\n",
        "        \"dep_heads\": dep_heads,\n",
        "        \"dep_labels\": dep_labels,\n",
        "        \"tokens\": tokens\n",
        "    }\n",
        "\n",
        "class ConlluDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, dep_label2id):\n",
        "        self.data = load_conllu_data(file_path)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.dep_label2id = dep_label2id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.data[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            example[\"sentence\"].split(),\n",
        "            is_split_into_words=True,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=128\n",
        "        )\n",
        "        label_ids = [self.dep_label2id.get(label, -100) for label in example[\"dep_labels\"]]\n",
        "\n",
        "        max_length = encoding[\"input_ids\"].shape[-1]\n",
        "        label_tensor = torch.full((max_length,), -100, dtype=torch.long)\n",
        "        label_tensor[:len(label_ids)] = torch.tensor(label_ids, dtype=torch.long)\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": label_tensor\n",
        "        }\n",
        "\n",
        "conllu_pth = \"ug_udt-ud-train.conllu\"\n",
        "dev_conllu   = \"ug_udt-ud-test.conllu\"\n",
        "\n",
        "# 1. Create dependency label mapping.\n",
        "dep_label2id, dep_id2label = create_dep_label_mapping(conllu_pth)\n",
        "print(f\"Initial Dependency Labels Mapping: {dep_label2id}\")\n",
        "\n",
        "# 2. Initialize tokenizer and create datasets.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "train_dataset = ConlluDataset(conllu_pth, tokenizer, dep_label2id)\n",
        "val_dataset = ConlluDataset(dev_conllu, tokenizer, dep_label2id)\n",
        "\n",
        "# 3. Update mapping based on training data to include unknown labels.\n",
        "all_train_labels = set()\n",
        "for example in train_dataset.data:\n",
        "    for label in example[\"dep_labels\"]:\n",
        "        all_train_labels.add(label)\n",
        "print(\"All labels in training set:\", all_train_labels)\n",
        "# Add any labels from the training set that are not in the mapping.\n",
        "for label in all_train_labels:\n",
        "    if label not in dep_label2id:\n",
        "        dep_label2id[label] = len(dep_label2id)\n",
        "dep_id2label = {v: k for k, v in dep_label2id.items()}\n",
        "NUM_DEP_LABELS = len(dep_label2id)\n",
        "print(\"Updated Dependency Labels Mapping:\", dep_label2id)\n",
        "\n",
        "# 4. Define compute_metrics.\n",
        "def compute_metrics(p):\n",
        "    predictions = np.argmax(p.predictions, axis=2)\n",
        "    true_labels = p.label_ids  # provided by collate_fn as \"labels\"\n",
        "    mask = true_labels != -100  # ignore padding tokens\n",
        "    y_true = true_labels[mask].flatten()\n",
        "    y_pred = predictions[mask].flatten()\n",
        "    return {\n",
        "        \"f1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred)\n",
        "    }\n",
        "\n",
        "# 5. Model Setup with AdapterFusion.\n",
        "model = AutoAdapterModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "# Load two adapters.\n",
        "model.load_adapter(\"la_s\", load_as=\"source_script\")\n",
        "model.load_adapter(\"la_t\", load_as=\"transliterated\")\n",
        "# Create and add adapter fusion.\n",
        "adapter_setup = Fuse(\"source_script\", \"transliterated\")\n",
        "model.add_adapter_fusion(adapter_setup)\n",
        "# Add dependency tagging head.\n",
        "model.add_tagging_head(\"dep_head\", num_labels=NUM_DEP_LABELS)\n",
        "# Activate fusion and the tagging head.\n",
        "model.train_adapter_fusion(adapter_setup)\n",
        "model.set_active_adapters(adapter_setup)\n",
        "if hasattr(model, \"set_active_head\"):\n",
        "    model.set_active_head(\"dep_head\")\n",
        "else:\n",
        "    model.active_head = \"dep_head\"\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
        "    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
        "    labels = torch.stack([item[\"labels\"] for item in batch])\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels\n",
        "    }\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    learning_rate=1e-4,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=1,\n",
        "    logging_dir=\"./logs\",\n",
        "    output_dir=\"./dep_fusion\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "trainer = AdapterTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "0IlO1SwsJC5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "outputId": "66f2dda4-ca56-4757-e6e1-5f7421d14652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Dependency Labels Mapping: {'acl': 0, 'advcl': 1, 'advmod': 2, 'amod': 3, 'ccomp': 4, 'compound': 5, 'conj': 6, 'cop': 7, 'det': 8, 'discourse': 9, 'dislocated': 10, 'flat': 11, 'nmod': 12, 'nsubj': 13, 'nummod': 14, 'obj': 15, 'obl': 16, 'orphan': 17, 'parataxis': 18, 'punct': 19, 'root': 20, 'vocative': 21, 'xcomp': 22}\n",
            "All labels in training set: {'acl', 'punct', 'nmod', 'xcomp', 'cop', 'conj', 'nummod', 'advcl', 'advmod', 'obl', 'discourse', 'dislocated', 'ccomp', 'flat', 'parataxis', 'root', 'vocative', 'orphan', 'compound', 'amod', 'obj', 'det', 'nsubj'}\n",
            "Updated Dependency Labels Mapping: {'acl': 0, 'advcl': 1, 'advmod': 2, 'amod': 3, 'ccomp': 4, 'compound': 5, 'conj': 6, 'cop': 7, 'det': 8, 'discourse': 9, 'dislocated': 10, 'flat': 11, 'nmod': 12, 'nsubj': 13, 'nummod': 14, 'obj': 15, 'obl': 16, 'orphan': 17, 'parataxis': 18, 'punct': 19, 'root': 20, 'vocative': 21, 'xcomp': 22}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertAdapterModel were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['heads.default.3.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [80/80 01:16, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.899213</td>\n",
              "      <td>0.077638</td>\n",
              "      <td>0.170213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.782184</td>\n",
              "      <td>0.070699</td>\n",
              "      <td>0.202128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.745335</td>\n",
              "      <td>0.071281</td>\n",
              "      <td>0.207447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.732247</td>\n",
              "      <td>0.071281</td>\n",
              "      <td>0.207447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.725943</td>\n",
              "      <td>0.071281</td>\n",
              "      <td>0.207447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.722145</td>\n",
              "      <td>0.070071</td>\n",
              "      <td>0.202128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.722893</td>\n",
              "      <td>0.070384</td>\n",
              "      <td>0.202128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.726014</td>\n",
              "      <td>0.070071</td>\n",
              "      <td>0.202128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.724043</td>\n",
              "      <td>0.070384</td>\n",
              "      <td>0.202128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.723901</td>\n",
              "      <td>0.070384</td>\n",
              "      <td>0.202128</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=80, training_loss=2.650547981262207, metrics={'train_runtime': 76.9272, 'train_samples_per_second': 2.08, 'train_steps_per_second': 1.04, 'total_flos': 13426854051840.0, 'train_loss': 2.650547981262207, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def infer_dependency_parsing(sentence, tokenizer, model, dep_id2label, max_length=128):\n",
        "    encoding = tokenizer(\n",
        "        sentence.split(),\n",
        "        is_split_into_words=True,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length\n",
        "    )\n",
        "    input_ids = encoding[\"input_ids\"].to(model.device)\n",
        "    attention_mask = encoding[\"attention_mask\"].to(model.device)\n",
        "\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1)[0]\n",
        "\n",
        "    word_ids = encoding.word_ids()\n",
        "    predicted_labels = {}\n",
        "    previous_word_idx = None\n",
        "    for idx, word_idx in enumerate(word_ids):\n",
        "        if word_idx is None:\n",
        "            continue\n",
        "        if word_idx != previous_word_idx:\n",
        "            label_id = predictions[idx].item()\n",
        "            predicted_labels[word_idx] = dep_id2label.get(label_id, \"UNK\")\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "    words = sentence.split()\n",
        "    results = []\n",
        "    for i, word in enumerate(words):\n",
        "        orig_token = word\n",
        "        parsed_token = word\n",
        "        pred_head = \"ROOT\" if i == 0 else words[i-1]\n",
        "        dep_label = predicted_labels.get(i, \"UNK\")\n",
        "        results.append((orig_token, parsed_token, pred_head, dep_label))\n",
        "    return results\n",
        "\n",
        "sample_sentence = \"نەشپۈت بەش يىلدا، ئۆرۈك تۆت يىلدا مېۋە بېرىدۇ دېگەننى ئاڭلىمىغانمىدىڭ ؟\"\n",
        "results = infer_dependency_parsing(sample_sentence, tokenizer, model, dep_id2label)\n",
        "\n",
        "print(\"Dependency Parsing Result:\")\n",
        "print(\"OrigToken\\t\\tParsedToken\\t\\tPredicted Head\\t\\tDependency Label\")\n",
        "for orig, parsed, head, label in results:\n",
        "    print(f\"{orig}\\t\\t{parsed}\\t\\t{head}\\t\\t{label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bk2p_nQNWb7o",
        "outputId": "030f45a9-2b21-4694-c0c8-9efbef94ecba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependency Parsing Result:\n",
            "OrigToken\t\tParsedToken\t\tPredicted Head\t\tDependency Label\n",
            "نەشپۈت\t\tنەشپۈت\t\tROOT\t\tpunct\n",
            "بەش\t\tبەش\t\tنەشپۈت\t\tpunct\n",
            "يىلدا،\t\tيىلدا،\t\tبەش\t\tpunct\n",
            "ئۆرۈك\t\tئۆرۈك\t\tيىلدا،\t\tpunct\n",
            "تۆت\t\tتۆت\t\tئۆرۈك\t\tpunct\n",
            "يىلدا\t\tيىلدا\t\tتۆت\t\tpunct\n",
            "مېۋە\t\tمېۋە\t\tيىلدا\t\tpunct\n",
            "بېرىدۇ\t\tبېرىدۇ\t\tمېۋە\t\tpunct\n",
            "دېگەننى\t\tدېگەننى\t\tبېرىدۇ\t\tpunct\n",
            "ئاڭلىمىغانمىدىڭ\t\tئاڭلىمىغانمىدىڭ\t\tدېگەننى\t\tpunct\n",
            "؟\t\t؟\t\tئاڭلىمىغانمىدىڭ\t\tpunct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EgiyKfOc8KLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unipelt POS tagging"
      ],
      "metadata": {
        "id": "nZrjCyYKkw0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "from peft import LoraConfig, PrefixTuningConfig, get_peft_model\n",
        "\n",
        "def load_conllu_data(file_path):\n",
        "    \"\"\"Loads CoNLL-U formatted data and extracts (tokens, upostags).\"\"\"\n",
        "    ud_treebank = []\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for tokenlist in parse_incr(f):\n",
        "            tokens, tags = [], []\n",
        "            for token in tokenlist:\n",
        "                tokens.append(token[\"form\"])\n",
        "                tags.append(token[\"upostag\"])\n",
        "            ud_treebank.append((tokens, tags))\n",
        "    return ud_treebank\n",
        "\n",
        "class POSDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, ud_treebank, tokenizer, max_length=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        all_tags = {t for _, tags in ud_treebank for t in tags}\n",
        "        self.label_map = {tag: i for i, tag in enumerate(sorted(all_tags))}\n",
        "        self.id2label = {i: tag for tag, i in self.label_map.items()}\n",
        "        self.features, self.labels = [], []\n",
        "\n",
        "        for tokens, tags in ud_treebank:\n",
        "            enc = tokenizer(\n",
        "                tokens,\n",
        "                is_split_into_words=True,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=max_length,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            word_ids = enc.word_ids()\n",
        "            label_ids = []\n",
        "            prev_word = None\n",
        "\n",
        "            for wid in word_ids:\n",
        "                if wid is None:\n",
        "                    label_ids.append(-100)\n",
        "                elif wid != prev_word:\n",
        "                    label_ids.append(self.label_map[tags[wid]])\n",
        "                else:\n",
        "                    label_ids.append(-100)\n",
        "                prev_word = wid\n",
        "\n",
        "            self.features.append({k: v.squeeze(0) for k, v in enc.items()})\n",
        "            self.labels.append(torch.tensor(label_ids))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.features[idx][\"input_ids\"],\n",
        "            \"attention_mask\": self.features[idx][\"attention_mask\"],\n",
        "            \"labels\": self.labels[idx]\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        \"input_ids\": torch.stack([b[\"input_ids\"] for b in batch]),\n",
        "        \"attention_mask\": torch.stack([b[\"attention_mask\"] for b in batch]),\n",
        "        \"labels\": torch.stack([b[\"labels\"] for b in batch])\n",
        "    }\n",
        "class UniPELTModel(nn.Module):\n",
        "    def __init__(self, num_labels):\n",
        "        super().__init__()\n",
        "        self.base_model = AutoModelForTokenClassification.from_pretrained(\n",
        "            \"bert-base-multilingual-cased\",\n",
        "            num_labels=num_labels,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "\n",
        "        # Create separate PEFT models\n",
        "        self.lora_model = self._create_lora_model()\n",
        "        self.prefix_model = self._create_prefix_model()\n",
        "        # self.base_model.add_adapter(\"la_t\", la_t_config)\n",
        "        # self.base_model.add_adapter(\"la_s\", la_s_config)\n",
        "        # Fusion components\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(2 * self.base_model.config.hidden_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, self.base_model.config.hidden_size)\n",
        "        )\n",
        "        # Final classifier\n",
        "        self.classifier = nn.Linear(\n",
        "            self.base_model.config.hidden_size,\n",
        "            num_labels\n",
        "        )\n",
        "\n",
        "    def _create_lora_model(self):\n",
        "        config = LoraConfig(\n",
        "            r=8,\n",
        "            lora_alpha=32,\n",
        "            target_modules=[\"query\", \"value\"],\n",
        "            bias=\"none\",\n",
        "            task_type=\"TOKEN_CLS\"\n",
        "        )\n",
        "        return get_peft_model(self.base_model, config)\n",
        "\n",
        "    def _create_prefix_model(self):\n",
        "        config = PrefixTuningConfig(\n",
        "            num_virtual_tokens=20,\n",
        "            task_type=\"TOKEN_CLS\"\n",
        "        )\n",
        "        return get_peft_model(self.base_model, config)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Get outputs from both PEFT models\n",
        "        lora_output = self.lora_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "\n",
        "        prefix_output = self.prefix_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "\n",
        "        # Fusion of last hidden states\n",
        "        fused = self.fusion(torch.cat([\n",
        "            lora_output.hidden_states[-1],\n",
        "            prefix_output.hidden_states[-1]\n",
        "        ], dim=-1))\n",
        "\n",
        "        return self.classifier(fused)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "train_data = load_conllu_data(\"/content/ug_udt-ud-train.conllu\")\n",
        "train_dataset = POSDataset(train_data, tokenizer)\n",
        "\n",
        "# Create model\n",
        "model = UniPELTModel(num_labels=len(train_dataset.label_map)).cuda()\n",
        "\n",
        "# Check trainable parameters\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lFf8sO1zZNG",
        "outputId": "72f132b6-bebb-42b3-811a-39d55e9c2f48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parameters: 974868\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/morningmoni/UniPELT.git\n",
        "%cd UniPELT\n",
        "!pip install -r requirements.txt\n",
        "# 1) Make sure PEFT is installed\n",
        "!pip install peft\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXBzoPnGlAji",
        "outputId": "ce128c27-465a-42a3-c616-4d70ce95b744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'UniPELT'...\n",
            "remote: Enumerating objects: 531, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 531 (delta 4), reused 0 (delta 0), pack-reused 518 (from 1)\u001b[K\n",
            "Receiving objects: 100% (531/531), 1.59 MiB | 3.65 MiB/s, done.\n",
            "Resolving deltas: 100% (209/209), done.\n",
            "/content/UniPELT\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd UniPELT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cwu7UeihpNMN",
        "outputId": "bc43b527-b802-46fe-fe0c-2c9dca2b57ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/UniPELT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "from peft import LoraConfig, PrefixTuningConfig, get_peft_model\n",
        "\n",
        "class UniPELTModel(nn.Module):\n",
        "    def __init__(self, num_labels):\n",
        "        super().__init__()\n",
        "        # Base model\n",
        "        self.base_model = AutoModelForTokenClassification.from_pretrained(\n",
        "            \"bert-base-multilingual-cased\",\n",
        "            num_labels=num_labels,\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "\n",
        "        # Freeze base model\n",
        "        for param in self.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Add LoRA\n",
        "        lora_config = LoraConfig(\n",
        "            r=8,\n",
        "            lora_alpha=32,\n",
        "            target_modules=[\"query\", \"value\"],\n",
        "            bias=\"none\",\n",
        "            task_type=\"TOKEN_CLS\",\n",
        "        )\n",
        "        self.lora_model = get_peft_model(self.base_model, lora_config)\n",
        "\n",
        "        # Add Prefix Tuning\n",
        "        prefix_config = PrefixTuningConfig(\n",
        "            num_virtual_tokens=20,\n",
        "            task_type=\"TOKEN_CLS\",\n",
        "        )\n",
        "        self.prefix_model = get_peft_model(self.base_model, prefix_config)\n",
        "\n",
        "        # Fusion components\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(2 * self.base_model.config.hidden_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        self.classifier = nn.Linear(\n",
        "            self.base_model.config.hidden_size,\n",
        "            num_labels\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Get outputs from both PEFT models\n",
        "        lora_output = self.lora_model.base_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "\n",
        "        prefix_output = self.prefix_model.base_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "\n",
        "        # Get last hidden states\n",
        "        h_lora = lora_output.hidden_states[-1]\n",
        "        h_prefix = prefix_output.hidden_states[-1]\n",
        "\n",
        "        # Compute gating weights\n",
        "        gate_weights = self.gate(torch.cat([h_lora, h_prefix], dim=-1))\n",
        "        fused = gate_weights[:,:,0:1] * h_lora + gate_weights[:,:,1:2] * h_prefix\n",
        "\n",
        "        return self.classifier(fused)\n",
        "\n",
        "# Initialize components\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "train_data = load_conllu_data(\"/content/ug_udt-ud-train.conllu\")\n",
        "\n",
        "\n",
        "train_dataset = POSDataset(train_data, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: {\n",
        "        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n",
        "        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n",
        "        'labels': torch.stack([x['labels'] for x in batch])\n",
        "    }\n",
        ")\n",
        "\n",
        "# Create model\n",
        "model = UniPELTModel(num_labels=len(train_dataset.label_map)).cuda()\n",
        "\n",
        "# Training setup\n",
        "optimizer = torch.optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=1e-4,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        inputs = {k: v.cuda() for k, v in batch.items()}\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
        "\n",
        "        loss = F.cross_entropy(\n",
        "            logits.view(-1, logits.size(-1)),\n",
        "            inputs[\"labels\"].view(-1),\n",
        "            ignore_index=-100\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        inputs = {k: v.cuda() for k, v in batch.items()}\n",
        "        logits = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        mask = inputs[\"labels\"] != -100\n",
        "        correct += (preds[mask] == inputs[\"labels\"][mask]).sum().item()\n",
        "        total += mask.sum().item()\n",
        "\n",
        "print(f\"Validation Accuracy: {correct/total:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DimGlHLyzZJQ",
        "outputId": "9ffe971a-8772-456c-c139-f9933f0df60d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 2.3726\n",
            "Epoch 2 Loss: 2.2726\n",
            "Epoch 3 Loss: 2.1711\n",
            "Epoch 4 Loss: 2.1106\n",
            "Epoch 5 Loss: 2.0550\n",
            "Epoch 6 Loss: 1.9820\n",
            "Epoch 7 Loss: 1.9213\n",
            "Epoch 8 Loss: 1.8696\n",
            "Epoch 9 Loss: 1.8251\n",
            "Epoch 10 Loss: 1.7668\n",
            "Epoch 11 Loss: 1.7376\n",
            "Epoch 12 Loss: 1.7326\n",
            "Epoch 13 Loss: 1.6895\n",
            "Epoch 14 Loss: 1.6791\n",
            "Epoch 15 Loss: 1.6125\n",
            "Epoch 16 Loss: 1.6097\n",
            "Epoch 17 Loss: 1.5775\n",
            "Epoch 18 Loss: 1.5642\n",
            "Epoch 19 Loss: 1.5448\n",
            "Epoch 20 Loss: 1.5044\n",
            "Epoch 21 Loss: 1.4973\n",
            "Epoch 22 Loss: 1.4730\n",
            "Epoch 23 Loss: 1.4683\n",
            "Epoch 24 Loss: 1.4503\n",
            "Epoch 25 Loss: 1.4317\n",
            "Epoch 26 Loss: 1.4140\n",
            "Epoch 27 Loss: 1.3920\n",
            "Epoch 28 Loss: 1.3930\n",
            "Epoch 29 Loss: 1.3707\n",
            "Epoch 30 Loss: 1.3839\n",
            "Validation Accuracy: 0.0968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_pos(sentence, model, tokenizer, id2label):\n",
        "    # Preprocess and tokenize\n",
        "    words = sentence.split()\n",
        "\n",
        "    # Tokenize with word IDs\n",
        "    encodings = tokenizer(\n",
        "        words,\n",
        "        is_split_into_words=True,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        logits = model(\n",
        "            encodings[\"input_ids\"].to(device),\n",
        "            encodings[\"attention_mask\"].to(device)\n",
        "        )\n",
        "\n",
        "    # Convert to predictions\n",
        "    predictions = logits.argmax(-1).squeeze().cpu().numpy()\n",
        "\n",
        "    # Align with original words\n",
        "    word_ids = encodings.word_ids()\n",
        "    pos_tags = []\n",
        "    current_word = None\n",
        "\n",
        "    for idx, word_id in enumerate(word_ids):\n",
        "        if word_id is None or word_id == current_word:\n",
        "            continue\n",
        "        current_word = word_id\n",
        "        pos_tags.append((words[word_id], id2label[predictions[idx]]))\n",
        "\n",
        "    return pos_tags\n",
        "\n",
        "# Usage\n",
        "test_sentence = \"نەشپۈت بەش يىلدا، ئۆرۈك تۆت يىلدا مېۋە بېرىدۇ دېگەننى ئاڭلىمىغانمىدىڭ؟\"\n",
        "predictions = predict_pos(test_sentence, model, tokenizer, train_dataset.id2label)\n",
        "\n",
        "# Print formatted results\n",
        "for word, tag in predictions:\n",
        "    print(f\"{word:20} -> {tag}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2x77T-MqzZGL",
        "outputId": "1fa4e164-d90a-4a53-ad05-2960c91f17c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "نەشپۈت               -> NOUN\n",
            "بەش                  -> NOUN\n",
            "يىلدا،               -> NOUN\n",
            "ئۆرۈك                -> NOUN\n",
            "تۆت                  -> NOUN\n",
            "يىلدا                -> NOUN\n",
            "مېۋە                 -> NOUN\n",
            "بېرىدۇ               -> NOUN\n",
            "دېگەننى              -> NOUN\n",
            "ئاڭلىمىغانمىدىڭ؟     -> NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P6HQlxdlzZC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HVJ9Nl0hzZAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependecy Parsing Unipelt"
      ],
      "metadata": {
        "id": "5FJBNtrAAS_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PrefixTuningConfig,\n",
        "    get_peft_model,\n",
        "    TaskType\n",
        ")\n",
        "\n",
        "# 1. Fix Dataset Class to Ensure Correct Keys\n",
        "class ConlluDataset(Dataset):\n",
        "\n",
        "    def __init__(self, file_path, tokenizer, dep_label2id, max_length=128):\n",
        "        self.sentences = self.load_conllu(file_path, dep_label2id)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.dep_label2id = dep_label2id\n",
        "    def __len__(self):  # ADD THIS METHOD\n",
        "        return len(self.sentences)\n",
        "    def load_conllu(self, file_path, dep_label2id):\n",
        "\n",
        "        sentences = []\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            sentence = []\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line == \"\":\n",
        "                    if sentence:\n",
        "                        sentences.append(sentence)\n",
        "                        sentence = []\n",
        "                elif not line.startswith(\"#\"):\n",
        "                    parts = line.split(\"\\t\")\n",
        "                    if len(parts) == 10:\n",
        "                        token_id, token, lemma, upos, xpos, feats, head, dep_rel, deps, misc = parts\n",
        "                        label = dep_label2id.get(dep_rel, -1)\n",
        "                        sentence.append((token, upos, int(head), label))\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "        return sentences\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        tokens = [word[0] for word in sentence]\n",
        "        dep_heads = [word[2] for word in sentence]\n",
        "        dep_labels = [word[3] for word in sentence]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            tokens,\n",
        "            is_split_into_words=True,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "            return_attention_mask=True,\n",
        "            return_offsets_mapping=True  # Correct parameter\n",
        "        )\n",
        "\n",
        "        # Get word IDs from the encoding\n",
        "        word_ids = encoding.word_ids(batch_index=0)\n",
        "\n",
        "        # Align dependencies\n",
        "        aligned_heads = []\n",
        "        aligned_labels = []\n",
        "        current_word_idx = -1\n",
        "\n",
        "        for wid in word_ids:\n",
        "            if wid is None:\n",
        "                aligned_heads.append(-1)\n",
        "                aligned_labels.append(-1)\n",
        "            elif wid != current_word_idx:\n",
        "                current_word_idx = wid\n",
        "                aligned_heads.append(dep_heads[wid])\n",
        "                aligned_labels.append(dep_labels[wid])\n",
        "            else:\n",
        "                aligned_heads.append(-1)\n",
        "                aligned_labels.append(-1)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"dep_heads\": torch.tensor(aligned_heads, dtype=torch.long),\n",
        "            \"dep_labels\": torch.tensor(aligned_labels, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# 2. Verify Data Loading\n",
        "def debug_dataset():\n",
        "    sample = ConlluDataset(conllu_pth, tokenizer, dep_label2id)[0]\n",
        "    print(\"Sample keys:\", sample.keys())  # Should show dep_heads and dep_labels\n",
        "    print(\"Dep heads shape:\", sample[\"dep_heads\"].shape)\n",
        "    print(\"Dep labels shape:\", sample[\"dep_labels\"].shape)\n",
        "\n",
        "debug_dataset()\n",
        "\n",
        "# 3. Fixed Collate Function\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n",
        "        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch]),\n",
        "        \"dep_heads\": torch.stack([item[\"dep_heads\"] for item in batch]),\n",
        "        \"dep_labels\": torch.stack([item[\"dep_labels\"] for item in batch])\n",
        "    }\n",
        "\n",
        "# 4. Initialize Data Loaders\n",
        "train_loader = DataLoader(\n",
        "    ConlluDataset(conllu_pth, tokenizer, dep_label2id),\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    ConlluDataset(\"/content/ug_udt-ud-test.conllu\", tokenizer, dep_label2id),\n",
        "    batch_size=4,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "class UniPELTDependencyParser(nn.Module):\n",
        "    def __init__(self, num_dep_labels, max_seq_length=128):\n",
        "        super().__init__()\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "        # Base model\n",
        "        self.base_model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "        # Freeze base model\n",
        "        for param in self.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Adapter configurations\n",
        "        lora_config = LoraConfig(\n",
        "            r=8,\n",
        "            lora_alpha=32,\n",
        "            target_modules=[\"query\", \"value\"],\n",
        "            bias=\"none\",\n",
        "            task_type=TaskType.FEATURE_EXTRACTION\n",
        "        )\n",
        "\n",
        "        prefix_config = PrefixTuningConfig(\n",
        "            num_virtual_tokens=20,\n",
        "            task_type=TaskType.FEATURE_EXTRACTION\n",
        "        )\n",
        "\n",
        "        # Create PEFT models\n",
        "        self.lora_model = get_peft_model(self.base_model, lora_config)\n",
        "        self.prefix_model = get_peft_model(self.base_model, prefix_config)\n",
        "\n",
        "        # Fusion components\n",
        "        self.fusion_gate = nn.Sequential(\n",
        "            nn.Linear(2 * self.base_model.config.hidden_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        # Dependency parsing heads\n",
        "        self.head_predictor = nn.Linear(\n",
        "            self.base_model.config.hidden_size,\n",
        "            self.max_seq_length  # Now outputs scores for each position\n",
        "        )\n",
        "        self.label_predictor = nn.Linear(\n",
        "            self.base_model.config.hidden_size,\n",
        "            num_dep_labels\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Get hidden states from both adapters\n",
        "        lora_output = self.lora_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "\n",
        "        prefix_output = self.prefix_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "\n",
        "        # Fusion of last hidden states\n",
        "        h_lora = lora_output.hidden_states[-1]\n",
        "        h_prefix = prefix_output.hidden_states[-1]\n",
        "        gate_weights = self.fusion_gate(torch.cat([h_lora, h_prefix], dim=-1))\n",
        "        fused = gate_weights[:,:,0:1] * h_lora + gate_weights[:,:,1:2] * h_prefix\n",
        "\n",
        "        # Predict dependencies\n",
        "        head_scores = self.head_predictor(fused)  # [batch_size, seq_len, max_seq_length]\n",
        "        label_logits = self.label_predictor(fused)  # [batch_size, seq_len, num_labels]\n",
        "\n",
        "        return head_scores, label_logits\n",
        "\n",
        "# Modified training function\n",
        "def train_dep_parser(model, dataloader, num_epochs=3, lr=1e-4):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            dep_heads = batch[\"dep_heads\"].to(device)\n",
        "            dep_labels = batch[\"dep_labels\"].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            head_scores, label_logits = model(input_ids, attention_mask)\n",
        "\n",
        "            # Calculate losses\n",
        "            head_loss = F.cross_entropy(\n",
        "                head_scores.view(-1, model.max_seq_length),\n",
        "                dep_heads.view(-1),\n",
        "                ignore_index=-1\n",
        "            )\n",
        "            label_loss = F.cross_entropy(\n",
        "                label_logits.view(-1, label_logits.size(-1)),\n",
        "                dep_labels.view(-1),\n",
        "                ignore_index=-1\n",
        "            )\n",
        "\n",
        "            total_loss = head_loss + label_loss\n",
        "            total_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += total_loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1} Loss: {total_loss/len(dataloader):.4f}\")\n",
        "# Modified evaluation function\n",
        "def evaluate_dep_parser(model, dataloader):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            dep_heads = batch[\"dep_heads\"].to(device)\n",
        "            dep_labels = batch[\"dep_labels\"].to(device)\n",
        "\n",
        "            head_scores, label_logits = model(input_ids, attention_mask)\n",
        "            head_preds = torch.argmax(head_scores, dim=-1)\n",
        "            label_preds = torch.argmax(label_logits, dim=-1)\n",
        "\n",
        "            # Mask for valid tokens\n",
        "            mask = (dep_labels != -1) & (dep_heads != -1)\n",
        "\n",
        "            # Calculate correct predictions\n",
        "            correct = ((head_preds == dep_heads) &\n",
        "                      (label_preds == dep_labels) &\n",
        "                      mask).sum().item()\n",
        "\n",
        "            total_correct += correct\n",
        "            total_tokens += mask.sum().item()\n",
        "\n",
        "    las = total_correct / total_tokens if total_tokens > 0 else 0\n",
        "    print(f\"Labeled Attachment Score (LAS): {las:.4f}\")\n",
        "    return las\n",
        "\n",
        "# Initialize model\n",
        "dep_label2id, dep_id2label = create_dep_label_mapping(conllu_pth)\n",
        "model = UniPELTDependencyParser(num_dep_labels=len(dep_label2id)).to(device)\n",
        "\n",
        "# Training\n",
        "# Now you can train normally\n",
        "# train_dep_parser(model, train_loader)\n",
        "train_dep_parser(model, train_loader, num_epochs=30)\n",
        "evaluate_dep_parser(model, val_loader)\n",
        "\n",
        "# Modified parsing function\n",
        "def parse_sentence(text, model, tokenizer, dep_id2label):\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Tokenize\n",
        "    encoding = tokenizer(\n",
        "        tokens,\n",
        "        is_split_into_words=True,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    # Get predictions\n",
        "    head_scores, label_logits = model(\n",
        "        encoding[\"input_ids\"].to(device),\n",
        "        encoding[\"attention_mask\"].to(device)\n",
        "    )\n",
        "\n",
        "    # Convert to predictions\n",
        "    head_preds = torch.argmax(head_scores, dim=-1).squeeze().cpu().numpy()\n",
        "    label_preds = torch.argmax(label_logits, dim=-1).squeeze().cpu().numpy()\n",
        "\n",
        "    # Align with original words\n",
        "    word_ids = encoding.word_ids()\n",
        "    predictions = []\n",
        "    current_word = None\n",
        "\n",
        "    for idx, word_id in enumerate(word_ids):\n",
        "        if word_id is None or word_id == current_word:\n",
        "            continue\n",
        "        current_word = word_id\n",
        "        predictions.append((\n",
        "            tokens[word_id],\n",
        "            head_preds[idx],\n",
        "            dep_id2label[label_preds[idx]]\n",
        "        ))\n",
        "\n",
        "    # Print results\n",
        "    print(\"Token\\tHead\\tDependency\")\n",
        "    for token, head, dep in predictions:\n",
        "        print(f\"{token}\\t{head}\\t{dep}\")\n",
        "\n",
        "# Example usage\n",
        "test_sentence = \"نەشپۈت بەش يىلدا، ئۆرۈك تۆت يىلدا مېۋە بېرىدۇ دېگەننى ئاڭلىمىغانمىدىڭ؟\"\n",
        "parse_sentence(test_sentence, model, tokenizer, dep_id2label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWeH1_aDzY9q",
        "outputId": "ceb99361-9abd-48a8-867a-7c433431361e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample keys: dict_keys(['input_ids', 'attention_mask', 'dep_heads', 'dep_labels'])\n",
            "Dep heads shape: torch.Size([128])\n",
            "Dep labels shape: torch.Size([128])\n",
            "Epoch 1 Loss: 3.9855\n",
            "Epoch 2 Loss: 3.8642\n",
            "Epoch 3 Loss: 3.8154\n",
            "Epoch 4 Loss: 3.7011\n",
            "Epoch 5 Loss: 3.6090\n",
            "Epoch 6 Loss: 3.5024\n",
            "Epoch 7 Loss: 3.3902\n",
            "Epoch 8 Loss: 3.3119\n",
            "Epoch 9 Loss: 3.2340\n",
            "Epoch 10 Loss: 3.1786\n",
            "Epoch 11 Loss: 3.1480\n",
            "Epoch 12 Loss: 3.0740\n",
            "Epoch 13 Loss: 3.0331\n",
            "Epoch 14 Loss: 2.9296\n",
            "Epoch 15 Loss: 2.8161\n",
            "Epoch 16 Loss: 2.8476\n",
            "Epoch 17 Loss: 2.7139\n",
            "Epoch 18 Loss: 2.6879\n",
            "Epoch 19 Loss: 2.7496\n",
            "Epoch 20 Loss: 2.6257\n",
            "Epoch 21 Loss: 2.6277\n",
            "Epoch 22 Loss: 2.5603\n",
            "Epoch 23 Loss: 2.5134\n",
            "Epoch 24 Loss: 2.4888\n",
            "Epoch 25 Loss: 2.5082\n",
            "Epoch 26 Loss: 2.4459\n",
            "Epoch 27 Loss: 2.4104\n",
            "Epoch 28 Loss: 2.5026\n",
            "Epoch 29 Loss: 2.3165\n",
            "Epoch 30 Loss: 2.3688\n",
            "Labeled Attachment Score (LAS): 0.0106\n",
            "Token\tHead\tDependency\n",
            "نەشپۈت\t9\tnsubj\n",
            "بەش\t9\tnsubj\n",
            "يىلدا،\t9\tpunct\n",
            "ئۆرۈك\t9\tnsubj\n",
            "تۆت\t9\tnummod\n",
            "يىلدا\t9\tpunct\n",
            "مېۋە\t9\tpunct\n",
            "بېرىدۇ\t9\tpunct\n",
            "دېگەننى\t9\tnsubj\n",
            "ئاڭلىمىغانمىدىڭ؟\t6\tpunct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rr26DppJzY6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "anNhv6BzuHnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unipelt plus pos tagging"
      ],
      "metadata": {
        "id": "GtCkWdHvAa7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "from peft import LoraConfig, PrefixTuningConfig, TaskType\n",
        "\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "class POSDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, ud_treebank, tokenizer, max_length=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.label_map = {}\n",
        "        self.id2label = {}\n",
        "        self.features = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Build label map\n",
        "        all_tags = {t for _, tags in ud_treebank for t in tags}\n",
        "        self.label_map = {tag: i for i, tag in enumerate(sorted(all_tags))}\n",
        "        self.id2label = {v: k for k, v in self.label_map.items()}\n",
        "\n",
        "        # Process sentences\n",
        "        for tokens, tags in ud_treebank:\n",
        "            encoding = tokenizer(\n",
        "                tokens,\n",
        "                is_split_into_words=True,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=max_length,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            # Align labels with subword tokens\n",
        "            word_ids = encoding.word_ids()\n",
        "            label_ids = []\n",
        "            current_word = None\n",
        "\n",
        "            for wid in word_ids:\n",
        "                if wid is None:  # Special tokens\n",
        "                    label_ids.append(-100)\n",
        "                elif wid != current_word:  # Start of new word\n",
        "                    label_ids.append(self.label_map[tags[wid]])\n",
        "                    current_word = wid\n",
        "                else:  # Subword continuation\n",
        "                    label_ids.append(-100)\n",
        "\n",
        "            self.features.append({\n",
        "                \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "                \"attention_mask\": encoding[\"attention_mask\"].squeeze(0)\n",
        "            })\n",
        "            self.labels.append(torch.tensor(label_ids))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.features[idx][\"input_ids\"],\n",
        "            \"attention_mask\": self.features[idx][\"attention_mask\"],\n",
        "            \"labels\": self.labels[idx]\n",
        "        }\n",
        "\n",
        "# Initialize dataset properly\n",
        "# train_dataset = POSDataset(train_data, tokenizer)\n",
        "def pos_collate_fn(batch):\n",
        "    return {\n",
        "        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n",
        "        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch]),\n",
        "        \"labels\": torch.stack([item[\"labels\"] for item in batch])\n",
        "    }\n",
        "\n",
        "# 3. Initialize Data Loaders with Correct Collate Function\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=pos_collate_fn\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,  # You'll need to create this\n",
        "    batch_size=4,\n",
        "    collate_fn=pos_collate_fn\n",
        ")\n",
        "\n",
        "class UniPELTFusionPlusPOS(nn.Module):\n",
        "    def __init__(self, num_labels):\n",
        "        super().__init__()\n",
        "        # Shared base model\n",
        "        self.base_model = AutoModelForTokenClassification.from_pretrained(\n",
        "            \"bert-base-multilingual-cased\",\n",
        "            ignore_mismatched_sizes=True\n",
        "        )\n",
        "\n",
        "        # Freeze base model\n",
        "        for param in self.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Initialize adapters for each script\n",
        "        self.script_models = nn.ModuleDict({\n",
        "            script: nn.ModuleDict({\n",
        "                'lora': get_peft_model(\n",
        "                    AutoModelForTokenClassification.from_pretrained(\n",
        "                        \"bert-base-multilingual-cased\",\n",
        "                        num_labels=num_labels,\n",
        "                        ignore_mismatched_sizes=True\n",
        "                    ),\n",
        "                    LoraConfig(\n",
        "                        r=8,\n",
        "                        lora_alpha=32,\n",
        "                        target_modules=[\"query\", \"value\"],\n",
        "                        bias=\"none\",\n",
        "                        task_type=TaskType.TOKEN_CLS\n",
        "                    )\n",
        "                ),\n",
        "                'prefix': get_peft_model(\n",
        "                    AutoModelForTokenClassification.from_pretrained(\n",
        "                        \"bert-base-multilingual-cased\",\n",
        "                        num_labels=num_labels,\n",
        "                        ignore_mismatched_sizes=True\n",
        "                    ),\n",
        "                    PrefixTuningConfig(\n",
        "                        num_virtual_tokens=20,\n",
        "                        task_type=TaskType.TOKEN_CLS\n",
        "                    )\n",
        "                )\n",
        "            }) for script in ['s', 't']\n",
        "        })\n",
        "\n",
        "        # Fusion components\n",
        "        self.fusion_gate = nn.Sequential(\n",
        "            nn.Linear(2 * self.base_model.config.hidden_size, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        # Final classifier\n",
        "        self.classifier = nn.Linear(self.base_model.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        script_outputs = []\n",
        "\n",
        "        # Get outputs for each script configuration\n",
        "        for script in ['s', 't']:\n",
        "            # Get LoRA output\n",
        "            lora_out = self.script_models[script]['lora'](\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                output_hidden_states=True\n",
        "            ).hidden_states[-1]\n",
        "\n",
        "            # Get Prefix output\n",
        "            prefix_out = self.script_models[script]['prefix'](\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                output_hidden_states=True\n",
        "            ).hidden_states[-1]\n",
        "\n",
        "            # Store combined script representation\n",
        "            script_outputs.append(lora_out + prefix_out)\n",
        "\n",
        "        # Fusion with residual connection\n",
        "        combined = torch.cat(script_outputs, dim=-1)\n",
        "        weights = self.fusion_gate(combined)\n",
        "        fused = (weights[:,:,0:1] * script_outputs[0] +\n",
        "                weights[:,:,1:2] * script_outputs[1] +\n",
        "                0.5 * (script_outputs[0] + script_outputs[1]))\n",
        "\n",
        "        return self.classifier(fused)\n",
        "\n",
        "# Initialize components\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "# train_data = load_conllu_data(\"/content/ug_udt-ud-train.conllu\")\n",
        "# train_dataset = POSDataset(train_data, tokenizer)\n",
        "\n",
        "# # Create model\n",
        "# model = UniPELTFusionPlusPOS(num_labels=len(train_dataset.label_map)).cuda()\n",
        "\n",
        "# Enhanced training with gradient clipping\n",
        "def train_pos_model(model, dataloader, num_epochs=30):\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()),\n",
        "        lr=2e-5,\n",
        "        weight_decay=0.01\n",
        "    )\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in dataloader:\n",
        "            inputs = {k: v.cuda() for k, v in batch.items()}\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
        "\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                inputs[\"labels\"].view(-1),\n",
        "                ignore_index=-100\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1} Loss: {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "def create_label_mapping(train_path, test_path=None):\n",
        "    \"\"\"Create label mapping considering both train and test data\"\"\"\n",
        "    labels = set()\n",
        "\n",
        "    # Process training data\n",
        "    with open(train_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if \"\\t\" in line:\n",
        "                parts = line.strip().split(\"\\t\")\n",
        "                if len(parts) >= 4:\n",
        "                    labels.add(parts[3])  # UPOS is in 4th column\n",
        "\n",
        "    # Process test data if provided\n",
        "    if test_path:\n",
        "        with open(test_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                if \"\\t\" in line:\n",
        "                    parts = line.strip().split(\"\\t\")\n",
        "                    if len(parts) >= 4:\n",
        "                        labels.add(parts[3])\n",
        "\n",
        "    # Create mappings\n",
        "    label2id = {tag: idx for idx, tag in enumerate(sorted(labels))}\n",
        "    id2label = {idx: tag for tag, idx in label2id.items()}\n",
        "    return label2id, id2label\n",
        "\n",
        "# Initialize with both train and test data\n",
        "label2id, id2label = create_label_mapping(\n",
        "    \"/content/ug_udt-ud-train.conllu\",\n",
        "    \"/content/ug_udt-ud-test.conllu\"\n",
        ")\n",
        "# Enhanced evaluation with F1 score\n",
        "def evaluate_pos(model, dataloader, id2label):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_tokens = 0\n",
        "    class_counts = {label: {'tp': 0, 'fp': 0, 'fn': 0} for label in id2label.values()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            inputs = {k: v.cuda() for k, v in batch.items()}\n",
        "            logits = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            mask = inputs[\"labels\"] != -100\n",
        "            valid_preds = preds[mask]\n",
        "            valid_labels = inputs[\"labels\"][mask]\n",
        "\n",
        "            # Handle possible unknown labels\n",
        "            for p, t in zip(valid_preds, valid_labels):\n",
        "                p_id = p.item()\n",
        "                t_id = t.item()\n",
        "\n",
        "                try:\n",
        "                    p_label = id2label[p_id]\n",
        "                except KeyError:\n",
        "                    p_label = \"UNK\"\n",
        "\n",
        "                try:\n",
        "                    t_label = id2label[t_id]\n",
        "                except KeyError:\n",
        "                    continue  # Skip invalid labels in ground truth\n",
        "\n",
        "                if p_label == t_label:\n",
        "                    class_counts[t_label]['tp'] += 1\n",
        "                else:\n",
        "                    if p_label in class_counts:\n",
        "                        class_counts[p_label]['fp'] += 1\n",
        "                    class_counts[t_label]['fn'] += 1\n",
        "\n",
        "            total_correct += (valid_preds == valid_labels).sum().item()\n",
        "            total_tokens += mask.sum().item()\n",
        "\n",
        "    # Rest of the evaluation logic remains the same...\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = total_correct / total_tokens\n",
        "    f1_scores = {}\n",
        "    for label, counts in class_counts.items():\n",
        "        precision = counts['tp'] / (counts['tp'] + counts['fp']) if (counts['tp'] + counts['fp']) > 0 else 0\n",
        "        recall = counts['tp'] / (counts['tp'] + counts['fn']) if (counts['tp'] + counts['fn']) > 0 else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        f1_scores[label] = f1\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"F1 Scores:\")\n",
        "    for label, score in f1_scores.items():\n",
        "        print(f\"{label}: {score:.4f}\")\n",
        "\n",
        "    return accuracy, f1_scores\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "# train_data = load_conllu_data(\"/content/ug_udt-ud-train.conllu\")\n",
        "# train_dataset = POSDataset(train_data, tokenizer)\n",
        "# Initialize label mappings with both datasets\n",
        "label2id, id2label = create_label_mapping(\n",
        "    \"/content/ug_udt-ud-train.conllu\",\n",
        "    \"/content/ug_udt-ud-test.conllu\"\n",
        ")\n",
        "\n",
        "# Initialize datasets with explicit label mapping\n",
        "train_dataset = POSDataset(\n",
        "    load_conllu_data(\"/content/ug_udt-ud-train.conllu\"),\n",
        "    tokenizer\n",
        ")\n",
        "\n",
        "test_dataset = POSDataset(\n",
        "    load_conllu_data(\"/content/ug_udt-ud-test.conllu\"),\n",
        "    tokenizer\n",
        ")\n",
        "# Create model\n",
        "model = UniPELTFusionPlusPOS(num_labels=len(train_dataset.label_map)).cuda()\n",
        "\n",
        "# Train and evaluate\n",
        "train_pos_model(model, train_loader, num_epochs=10)\n",
        "\n",
        "\n",
        "evaluate_pos(model, test_loader, train_dataset.id2label)\n",
        "\n",
        "# Usage remains the same as original\n",
        "test_sentence = \"نەشپۈت بەش يىلدا، ئۆرۈك تۆت يىلدا مېۋە بېرىدۇ دېگەننى ئاڭلىمىغانمىدىڭ؟\"\n",
        "predictions = predict_pos(test_sentence, model, tokenizer, train_dataset.id2label)\n",
        "\n",
        "# Print results\n",
        "for word, tag in predictions:\n",
        "    print(f\"{word:20} -> {tag}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4ahPEJdAlJV",
        "outputId": "da59595e-f51e-4a3e-8608-005da3bbd8d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 2.0484\n",
            "Epoch 2 Loss: 2.0020\n",
            "Epoch 3 Loss: 1.9010\n",
            "Epoch 4 Loss: 1.9064\n",
            "Epoch 5 Loss: 1.8183\n",
            "Epoch 6 Loss: 1.8001\n",
            "Epoch 7 Loss: 1.8067\n",
            "Epoch 8 Loss: 1.7655\n",
            "Epoch 9 Loss: 1.7183\n",
            "Epoch 10 Loss: 1.6385\n",
            "Epoch 11 Loss: 1.6671\n",
            "Epoch 12 Loss: 1.6087\n",
            "Epoch 13 Loss: 1.6569\n",
            "Epoch 14 Loss: 1.6354\n",
            "Epoch 15 Loss: 1.5842\n",
            "Accuracy: 0.0302\n",
            "F1 Scores:\n",
            "ADJ: 0.0000\n",
            "ADV: 0.0000\n",
            "AUX: 0.0000\n",
            "INTJ: 0.0000\n",
            "NOUN: 0.0292\n",
            "NUM: 0.0000\n",
            "PRON: 0.0204\n",
            "PROPN: 0.0000\n",
            "PUNCT: 0.0588\n",
            "VERB: 0.1000\n",
            "نەشپۈت               -> NOUN\n",
            "بەش                  -> NOUN\n",
            "يىلدا،               -> NOUN\n",
            "ئۆرۈك                -> NOUN\n",
            "تۆت                  -> NOUN\n",
            "يىلدا                -> NOUN\n",
            "مېۋە                 -> NOUN\n",
            "بېرىدۇ               -> NOUN\n",
            "دېگەننى              -> NOUN\n",
            "ئاڭلىمىغانمىدىڭ؟     -> NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rtyscDYjCHg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unipelt plus dependecy parsing"
      ],
      "metadata": {
        "id": "s6WYf2yvAl5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from peft import LoraConfig, PrefixTuningConfig, get_peft_model, TaskType\n",
        "\n",
        "class UniPELTFusionPlus(nn.Module):\n",
        "    def __init__(self, hidden_size, num_scripts=2):\n",
        "        super().__init__()\n",
        "        # Enhanced fusion with layer normalization\n",
        "        self.lora_fusion = nn.Sequential(\n",
        "            nn.Linear(hidden_size * num_scripts, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, num_scripts),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        self.prefix_fusion = nn.Sequential(\n",
        "            nn.Linear(hidden_size * num_scripts, 256),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, num_scripts),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, script_components):\n",
        "        # Dynamic fusion with residual connection\n",
        "        lora_s, lora_t = script_components['lora']\n",
        "        prefix_s, prefix_t = script_components['prefix']\n",
        "\n",
        "        # LoRA fusion\n",
        "        lora_cat = torch.cat([lora_s, lora_t], dim=-1)\n",
        "        lora_weights = self.lora_fusion(lora_cat)\n",
        "        fused_lora = lora_weights[:,:,0:1]*lora_s + lora_weights[:,:,1:2]*lora_t\n",
        "\n",
        "        # Prefix fusion\n",
        "        prefix_cat = torch.cat([prefix_s, prefix_t], dim=-1)\n",
        "        prefix_weights = self.prefix_fusion(prefix_cat)\n",
        "        fused_prefix = prefix_weights[:,:,0:1]*prefix_s + prefix_weights[:,:,1:2]*prefix_t\n",
        "\n",
        "        return fused_lora + fused_prefix + 0.5*(lora_s + lora_t + prefix_s + prefix_t)\n",
        "\n",
        "class UniPELTDependencyParser(nn.Module):\n",
        "    def __init__(self, num_dep_labels, max_seq_length=128):\n",
        "        super().__init__()\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "        # Shared base model\n",
        "        self.base_model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "        # Freeze base model\n",
        "        for param in self.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Script-specific adapters\n",
        "        self.script_configs = nn.ModuleDict({\n",
        "            script: nn.ModuleDict({\n",
        "                'lora': get_peft_model(\n",
        "                    AutoModel.from_pretrained(\"bert-base-multilingual-cased\"),\n",
        "                    LoraConfig(\n",
        "                        r=8,\n",
        "                        lora_alpha=32,\n",
        "                        target_modules=[\"query\", \"value\"],\n",
        "                        bias=\"none\",\n",
        "                        task_type=TaskType.FEATURE_EXTRACTION\n",
        "                    )\n",
        "                ),\n",
        "                'prefix': get_peft_model(\n",
        "                    AutoModel.from_pretrained(\"bert-base-multilingual-cased\"),\n",
        "                    PrefixTuningConfig(\n",
        "                        num_virtual_tokens=20,\n",
        "                        task_type=TaskType.FEATURE_EXTRACTION,\n",
        "                        prefix_projection=False\n",
        "                    )\n",
        "                )\n",
        "            }) for script in ['s', 't']\n",
        "        })\n",
        "\n",
        "        # Fusion and prediction\n",
        "        self.fusion = UniPELTFusionPlus(self.base_model.config.hidden_size)\n",
        "        self.head_predictor = nn.Linear(self.base_model.config.hidden_size, max_seq_length)\n",
        "        self.label_predictor = nn.Linear(self.base_model.config.hidden_size, num_dep_labels)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        script_outputs = {'lora': [], 'prefix': []}\n",
        "\n",
        "        for script in ['s', 't']:\n",
        "            # LoRA pathway\n",
        "            lora_out = self.script_configs[script]['lora'](\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                output_hidden_states=True\n",
        "            ).hidden_states[-1]\n",
        "\n",
        "            # Prefix pathway\n",
        "            prefix_out = self.script_configs[script]['prefix'](\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                output_hidden_states=True\n",
        "            ).hidden_states[-1]\n",
        "\n",
        "            script_outputs['lora'].append(lora_out)\n",
        "            script_outputs['prefix'].append(prefix_out)\n",
        "\n",
        "        # Fuse representations\n",
        "        fused = self.fusion(script_outputs)\n",
        "        fused = self.dropout(fused)\n",
        "\n",
        "        # Predict dependencies\n",
        "        head_logits = self.head_predictor(fused)\n",
        "        label_logits = self.label_predictor(fused)\n",
        "\n",
        "        return head_logits, label_logits\n",
        "\n",
        "# Initialize model with proper parameter isolation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dep_label2id, dep_id2label = create_dep_label_mapping(conllu_pth)\n",
        "model = UniPELTDependencyParser(\n",
        "    num_dep_labels=len(dep_label2id),\n",
        "    max_seq_length=128\n",
        ").to(device)\n",
        "\n",
        "# Enhanced training with gradient accumulation\n",
        "def train_unipelt_fusion(model, dataloader, num_epochs=3, accum_steps=4):\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()),\n",
        "        lr=2e-5,\n",
        "        weight_decay=0.01\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer,\n",
        "        T_max=num_epochs*len(dataloader)//accum_steps\n",
        "    )\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "            head_logits, label_logits = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
        "\n",
        "            # Calculate losses\n",
        "            head_loss = F.cross_entropy(\n",
        "                head_logits.view(-1, model.max_seq_length),\n",
        "                inputs[\"dep_heads\"].view(-1),\n",
        "                ignore_index=-1\n",
        "            )\n",
        "            label_loss = F.cross_entropy(\n",
        "                label_logits.view(-1, label_logits.size(-1)),\n",
        "                inputs[\"dep_labels\"].view(-1),\n",
        "                ignore_index=-1\n",
        "            )\n",
        "            loss = (head_loss + label_loss) / accum_steps\n",
        "            loss.backward()\n",
        "\n",
        "            if (i+1) % accum_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            total_loss += loss.item() * accum_steps\n",
        "\n",
        "        print(f\"Epoch {epoch+1} Loss: {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "# Enhanced evaluation with tree validation\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    metrics = {'uas': 0, 'las': 0, 'total': 0}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "            head_logits, label_logits = model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
        "\n",
        "            # Get predictions\n",
        "            head_preds = torch.argmax(head_logits, dim=-1)\n",
        "            label_preds = torch.argmax(label_logits, dim=-1)\n",
        "\n",
        "            # Calculate valid tokens\n",
        "            mask = (inputs[\"dep_labels\"] != -1) & (inputs[\"dep_heads\"] != -1)\n",
        "            metrics['total'] += mask.sum().item()\n",
        "\n",
        "            # Calculate matches\n",
        "            correct_head = (head_preds[mask] == inputs[\"dep_heads\"][mask])\n",
        "            correct_label = (label_preds[mask] == inputs[\"dep_labels\"][mask])\n",
        "            metrics['uas'] += correct_head.sum().item()\n",
        "            metrics['las'] += (correct_head & correct_label).sum().item()\n",
        "\n",
        "    uas = metrics['uas'] / metrics['total']\n",
        "    las = metrics['las'] / metrics['total']\n",
        "    print(f\"Unlabeled Attachment Score: {uas:.4f}\")\n",
        "    print(f\"Labeled Attachment Score: {las:.4f}\")\n",
        "    return uas, las\n",
        "# Initialize data loaders\n",
        "train_loader = DataLoader(\n",
        "    ConlluDataset(conllu_pth, tokenizer, dep_label2id),\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    ConlluDataset(\"/content/ug_udt-ud-dev.conllu\", tokenizer, dep_label2id),\n",
        "    batch_size=2,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "gtr2YehYAohV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate\n",
        "train_unipelt_fusion(model, train_loader, num_epochs=100)\n",
        "evaluate_model(model, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUXfBqlFu2vq",
        "outputId": "2d952877-2dd0-4b05-8095-57553de2e37a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 9.0337\n",
            "Epoch 2 Loss: 8.8239\n",
            "Epoch 3 Loss: 8.7601\n",
            "Epoch 4 Loss: 8.8130\n",
            "Epoch 5 Loss: 8.6873\n",
            "Epoch 6 Loss: 8.6392\n",
            "Epoch 7 Loss: 8.5404\n",
            "Epoch 8 Loss: 8.4710\n",
            "Epoch 9 Loss: 8.4333\n",
            "Epoch 10 Loss: 8.4580\n",
            "Epoch 11 Loss: 8.3892\n",
            "Epoch 12 Loss: 8.2598\n",
            "Epoch 13 Loss: 8.1667\n",
            "Epoch 14 Loss: 8.1342\n",
            "Epoch 15 Loss: 7.9716\n",
            "Epoch 16 Loss: 8.0880\n",
            "Epoch 17 Loss: 7.9655\n",
            "Epoch 18 Loss: 8.0396\n",
            "Epoch 19 Loss: 7.8549\n",
            "Epoch 20 Loss: 7.8566\n",
            "Epoch 21 Loss: 7.7258\n",
            "Epoch 22 Loss: 7.7988\n",
            "Epoch 23 Loss: 7.6627\n",
            "Epoch 24 Loss: 7.6241\n",
            "Epoch 25 Loss: 7.5606\n",
            "Epoch 26 Loss: 7.4905\n",
            "Epoch 27 Loss: 7.4605\n",
            "Epoch 28 Loss: 7.4145\n",
            "Epoch 29 Loss: 7.4700\n",
            "Epoch 30 Loss: 7.3573\n",
            "Epoch 31 Loss: 7.2174\n",
            "Epoch 32 Loss: 7.2355\n",
            "Epoch 33 Loss: 7.2675\n",
            "Epoch 34 Loss: 7.1396\n",
            "Epoch 35 Loss: 7.1354\n",
            "Epoch 36 Loss: 7.0493\n",
            "Epoch 37 Loss: 7.0380\n",
            "Epoch 38 Loss: 7.0553\n",
            "Epoch 39 Loss: 6.9847\n",
            "Epoch 40 Loss: 6.9403\n",
            "Epoch 41 Loss: 6.8762\n",
            "Epoch 42 Loss: 6.8361\n",
            "Epoch 43 Loss: 6.7998\n",
            "Epoch 44 Loss: 6.9041\n",
            "Epoch 45 Loss: 6.8001\n",
            "Epoch 46 Loss: 6.7005\n",
            "Epoch 47 Loss: 6.6474\n",
            "Epoch 48 Loss: 6.8210\n",
            "Epoch 49 Loss: 6.6142\n",
            "Epoch 50 Loss: 6.5164\n",
            "Epoch 51 Loss: 6.5807\n",
            "Epoch 52 Loss: 6.5943\n",
            "Epoch 53 Loss: 6.6082\n",
            "Epoch 54 Loss: 6.4713\n",
            "Epoch 55 Loss: 6.5549\n",
            "Epoch 56 Loss: 6.5338\n",
            "Epoch 57 Loss: 6.4949\n",
            "Epoch 58 Loss: 6.5734\n",
            "Epoch 59 Loss: 6.4616\n",
            "Epoch 60 Loss: 6.5115\n",
            "Epoch 61 Loss: 6.4138\n",
            "Epoch 62 Loss: 6.4071\n",
            "Epoch 63 Loss: 6.2131\n",
            "Epoch 64 Loss: 6.3997\n",
            "Epoch 65 Loss: 6.4599\n",
            "Epoch 66 Loss: 6.3249\n",
            "Epoch 67 Loss: 6.3434\n",
            "Epoch 68 Loss: 6.3571\n",
            "Epoch 69 Loss: 6.2145\n",
            "Epoch 70 Loss: 6.3875\n",
            "Epoch 71 Loss: 6.3080\n",
            "Epoch 72 Loss: 6.2925\n",
            "Epoch 73 Loss: 6.3277\n",
            "Epoch 74 Loss: 6.3141\n",
            "Epoch 75 Loss: 6.2814\n",
            "Epoch 76 Loss: 6.1211\n",
            "Epoch 77 Loss: 6.2012\n",
            "Epoch 78 Loss: 6.2827\n",
            "Epoch 79 Loss: 6.1816\n",
            "Epoch 80 Loss: 6.2001\n",
            "Epoch 81 Loss: 6.2361\n",
            "Epoch 82 Loss: 6.1794\n",
            "Epoch 83 Loss: 6.1782\n",
            "Epoch 84 Loss: 6.1206\n",
            "Epoch 85 Loss: 6.1747\n",
            "Epoch 86 Loss: 6.1696\n",
            "Epoch 87 Loss: 6.2135\n",
            "Epoch 88 Loss: 6.2291\n",
            "Epoch 89 Loss: 6.1453\n",
            "Epoch 90 Loss: 6.0595\n",
            "Epoch 91 Loss: 6.1836\n",
            "Epoch 92 Loss: 6.2098\n",
            "Epoch 93 Loss: 6.2299\n",
            "Epoch 94 Loss: 6.1857\n",
            "Epoch 95 Loss: 6.1905\n",
            "Epoch 96 Loss: 6.1576\n",
            "Epoch 97 Loss: 6.1518\n",
            "Epoch 98 Loss: 6.1832\n",
            "Epoch 99 Loss: 6.1902\n",
            "Epoch 100 Loss: 6.2102\n",
            "Unlabeled Attachment Score: 0.0680\n",
            "Labeled Attachment Score: 0.0097\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.06796116504854369, 0.009708737864077669)"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_sentence_dep_fusion(model, tokenizer, sentence, dep_id2label, max_length=128):\n",
        "    # Tokenize input\n",
        "    words = sentence.split()\n",
        "    encoding = tokenizer(\n",
        "        words,\n",
        "        is_split_into_words=True,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\",\n",
        "        return_offsets_mapping=True\n",
        "    )\n",
        "\n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        head_logits, label_logits = model(\n",
        "            encoding[\"input_ids\"].to(device),\n",
        "            encoding[\"attention_mask\"].to(device)\n",
        "        )\n",
        "\n",
        "    # Convert to predictions\n",
        "    head_preds = torch.argmax(head_logits, dim=-1).squeeze().cpu().numpy()\n",
        "    label_preds = torch.argmax(label_logits, dim=-1).squeeze().cpu().numpy()\n",
        "\n",
        "    # Align with original words\n",
        "    word_ids = encoding.word_ids()\n",
        "    dependencies = []\n",
        "    current_word = None\n",
        "\n",
        "    for idx, wid in enumerate(word_ids):\n",
        "        if wid is None or wid == current_word:\n",
        "            continue  # Skip special tokens and subwords\n",
        "\n",
        "        current_word = wid\n",
        "        try:\n",
        "            dep_entry = (\n",
        "                words[wid],  # Token\n",
        "                int(head_preds[idx]),  # Predicted head\n",
        "                dep_id2label[label_preds[idx]]  # Dependency label\n",
        "            )\n",
        "            dependencies.append(dep_entry)\n",
        "        except IndexError:\n",
        "            continue\n",
        "\n",
        "    # Validate tree structure\n",
        "    root = [i for i, (_, head, _) in enumerate(dependencies) if head == 0]\n",
        "    if not root:\n",
        "        dependencies[0] = (dependencies[0][0], 0, \"root\")\n",
        "\n",
        "    return dependencies\n",
        "\n",
        "# Example usage\n",
        "test_sentence = \"نەشپۈت بەش يىلدا، ئۆرۈك تۆت يىلدا مېۋە بېرىدۇ دېگەننى ئاڭلىمىغانمىدىڭ؟\"\n",
        "dependencies = parse_sentence_dep_fusion(model, tokenizer, test_sentence, dep_id2label)\n",
        "\n",
        "print(\"Token\\tHead\\tDependency\")\n",
        "for i, (token, head, label) in enumerate(dependencies):\n",
        "    print(f\"{i+1}\\t{token}\\t{head}\\t{label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TPbkkWRDV0c",
        "outputId": "ca4915ba-d6f0-48c1-be52-b516cd34146e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token\tHead\tDependency\n",
            "1\tنەشپۈت\t15\tvocative\n",
            "2\tبەش\t15\tvocative\n",
            "3\tيىلدا،\t12\tvocative\n",
            "4\tئۆرۈك\t15\tvocative\n",
            "5\tتۆت\t0\tvocative\n",
            "6\tيىلدا\t58\tobl\n",
            "7\tمېۋە\t42\tvocative\n",
            "8\tبېرىدۇ\t15\tvocative\n",
            "9\tدېگەننى\t15\tvocative\n",
            "10\tئاڭلىمىغانمىدىڭ؟\t58\torphan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hRqfUYktKJKg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}